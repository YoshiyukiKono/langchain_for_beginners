{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoshiyukiKono/langchain_for_beginners/blob/main/work/1a_RAG_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf\n",
        "\n",
        "## パッケージインストール"
      ],
      "metadata": {
        "id": "YODRRq-0Y4Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.3.5 tiktoken==0.5.1 cohere==4.36\n",
        "!pip install langchain==0.0.340"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMMMfJaKo4Ww",
        "outputId": "f5dad076-df7c-4405-ba28-dd6645d78a12"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.3.5\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken==0.5.1\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere==4.36\n",
            "  Downloading cohere-4.36-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.3.5) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.3.5)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (4.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (3.8.6)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere==4.36)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere==4.36)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (6.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.5) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.3.5)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.5)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere==4.36) (3.17.0)\n",
            "Installing collected packages: h11, fastavro, backoff, tiktoken, httpcore, httpx, cohere, openai\n",
            "Successfully installed backoff-2.2.1 cohere-4.36 fastavro-1.8.2 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5 tiktoken-0.5.1\n",
            "Collecting langchain==0.0.340\n",
            "  Downloading langchain-0.0.340-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.340)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.340)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.340)\n",
            "  Downloading langsmith-0.0.67-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.340)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.340) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.340) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.340 langsmith-0.0.67 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cassandra-driver==3.28.0 cassio==0.1.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k10ziT0zRYw",
        "outputId": "2730842b-ad83-4970-815c-98a56c48a216"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cassandra-driver==3.28.0\n",
            "  Downloading cassandra_driver-3.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cassio==0.1.3\n",
            "  Downloading cassio-0.1.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver==3.28.0) (1.16.0)\n",
            "Collecting geomet<0.3,>=0.1 (from cassandra-driver==3.28.0)\n",
            "  Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from cassio==0.1.3) (1.23.5)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.10/dist-packages (from cassio==0.1.3) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver==3.28.0) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (2023.7.22)\n",
            "Installing collected packages: geomet, cassandra-driver, cassio\n",
            "Successfully installed cassandra-driver-3.28.0 cassio-0.1.3 geomet-0.2.1.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf==3.17.0"
      ],
      "metadata": {
        "id": "oX33RRkrY2WY",
        "outputId": "edead122-9b6d-44fc-c269-21b506f07367",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf==3.17.0\n",
            "  Downloading pypdf-3.17.0-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI 接続"
      ],
      "metadata": {
        "id": "IAAChGc-tpsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "WLkDfg9_dB_l",
        "outputId": "f7cbf587-fe77-44b6-e410-b7f4408d7831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "client.models.list()"
      ],
      "metadata": {
        "id": "TGmRlMyBtZ4n",
        "outputId": "78062773-f8d8-42f5-ee52-4a6628102d16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncPage[Model](data=[Model(id='text-search-babbage-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'), Model(id='curie-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-003', created=1669599635, object='model', owned_by='openai-internal'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='text-search-babbage-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='babbage', created=1649358449, object='model', owned_by='openai'), Model(id='babbage-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-babbage-001', created=1649364043, object='model', owned_by='openai'), Model(id='text-similarity-davinci-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='davinci-similarity', created=1651172509, object='model', owned_by='openai-dev'), Model(id='code-davinci-edit-001', created=1649880484, object='model', owned_by='openai'), Model(id='curie-similarity', created=1651172510, object='model', owned_by='openai-dev'), Model(id='babbage-search-document', created=1651172510, object='model', owned_by='openai-dev'), Model(id='curie-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-ada-doc-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='davinci-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='text-similarity-babbage-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-search-davinci-doc-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='babbage-similarity', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'), Model(id='davinci-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-similarity-curie-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='text-davinci-001', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-davinci-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='ada-search-document', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-code-search-code', created=1651172505, object='model', owned_by='openai-dev'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='gpt-4-vision-preview', created=1698894917, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='gpt-4-0314', created=1687882410, object='model', owned_by='openai'), Model(id='davinci-search-document', created=1651172509, object='model', owned_by='openai-dev'), Model(id='curie-search-document', created=1651172508, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-code', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'), Model(id='text-search-ada-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='code-search-ada-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-text', created=1651172509, object='model', owned_by='openai-dev'), Model(id='code-search-babbage-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='ada-code-search-text', created=1651172510, object='model', owned_by='openai-dev'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='text-search-curie-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-002', created=1649880484, object='model', owned_by='openai'), Model(id='text-davinci-edit-001', created=1649809179, object='model', owned_by='openai'), Model(id='code-search-babbage-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada', created=1649357491, object='model', owned_by='openai'), Model(id='text-ada-001', created=1649364042, object='model', owned_by='openai'), Model(id='ada-similarity', created=1651172507, object='model', owned_by='openai-dev'), Model(id='code-search-ada-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='text-similarity-ada-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='canary-whisper', created=1699656801, object='model', owned_by='system'), Model(id='text-search-curie-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-curie-001', created=1649364043, object='model', owned_by='openai'), Model(id='curie', created=1649359874, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai'), Model(id='canary-tts', created=1699492935, object='model', owned_by='system'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'), Model(id='davinci', created=1649359874, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-16k-0613', created=1685474247, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system')], object='list')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDFファイルの準備"
      ],
      "metadata": {
        "id": "erSlFwzJo71s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"./DataStax_Vector-Search.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "uqx1XkeMZCMd",
        "outputId": "a66f0df3-e720-4816-ca37-ec711eb595c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-b2a8f3a4bb8f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./aDataStax_intro.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;34m\"pypdf package not found, please install it with \"\u001b[0m \u001b[0;34m\"`pip install pypdf`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             )\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File path %s is not a valid file or url\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File path ./aDataStax_intro.pdf is not a valid file or url"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "id": "QzK8Pk4RZY5I",
        "outputId": "fa6adaaf-7f8a-49ea-b205-ea60a219b7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Astra DB接続\n"
      ],
      "metadata": {
        "id": "5GmUfFL0YCtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O secure-connect-demo.zip \"https://datastax-cluster-config-...\""
      ],
      "metadata": {
        "id": "IUm51RWBJiU9",
        "outputId": "99d4fd11-d545-4253-b1bf-502da955faa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-30 00:40:33--  https://datastax-cluster-config-prod.s3.us-east-2.amazonaws.com/2c170f07-40f1-4550-b361-6b2c8dabb94f-1/secure-connect-demo.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA2AIQRQ76S2JCB77W%2F20231130%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20231130T003839Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=d5253ef1bd43b488ceae3db174eb2d789b6951ecf8571d894a3ee9ce2be5ee59\n",
            "Resolving datastax-cluster-config-prod.s3.us-east-2.amazonaws.com (datastax-cluster-config-prod.s3.us-east-2.amazonaws.com)... 52.219.110.194, 52.219.178.42, 52.219.94.66, ...\n",
            "Connecting to datastax-cluster-config-prod.s3.us-east-2.amazonaws.com (datastax-cluster-config-prod.s3.us-east-2.amazonaws.com)|52.219.110.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12254 (12K) [application/zip]\n",
            "Saving to: ‘secure-connect-demo.zip’\n",
            "\n",
            "secure-connect-demo 100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-30 00:40:33 (108 MB/s) - ‘secure-connect-demo.zip’ saved [12254/12254]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SECURE_CONNECT_BUNDLE_PATH = 'secure-connect-demo.zip'"
      ],
      "metadata": {
        "id": "qGzwcnhfIROt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "ASTRA_CLIENT_ID = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVlMnfyJJVUb",
        "outputId": "927075df-bf1a-4581-f469-91c021c49e81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ASTRA_CLIENT_SECRET = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvKJFm2vJgoU",
        "outputId": "92378afb-c343-4750-9e16-56fa8d38123c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "cloud_config= {\n",
        "  'secure_connect_bundle': SECURE_CONNECT_BUNDLE_PATH\n",
        "}\n",
        "auth_provider = PlainTextAuthProvider(ASTRA_CLIENT_ID, ASTRA_CLIENT_SECRET)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrGG5yAEJpnJ",
        "outputId": "e2e5f908-8a7a-4ca8-aaf0-50861d360b99"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:f46893af-3674-4086-ad1f-fb43b657e4df. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:f46893af-3674-4086-ad1f-fb43b657e4df. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(132421572132528) 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:f46893af-3674-4086-ad1f-fb43b657e4df> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:f46893af-3674-4086-ad1f-fb43b657e4df. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0.11-13697dcfc157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain\n",
        "\n",
        "### PDFファイルのスプリット"
      ],
      "metadata": {
        "id": "ZlSnousTt2cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Cassandra\n",
        "from langchain.document_loaders import TextLoader"
      ],
      "metadata": {
        "id": "FCsCyZLJcTgF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "Xv2mLw5ece87"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))\n",
        "print(docs[0])\n",
        "print(docs[len(docs)-1])"
      ],
      "metadata": {
        "id": "EpGh_t-UuD9x",
        "outputId": "8a0ce884-038d-47e6-c5d0-091acc7271c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "page_content='ホワイトペーパー\\n生成 AIアプリ\\nのための\\nベクトル検索\\nAIアプリケーション開発にベクトル検索を活用す\\nるための開発者/アーキテクト向けガイド\\nこのドキュメントは、生成 AIアプリケーションを企業独\\n自のデータと組み合わせて、設計・構築しようとしてい\\nる全ての方のためのガイドです。組織が理解すべき重\\n要な概念と考慮事項を取り扱うだけでなく、ベクトル検\\n索を用いて、 LLMの持つ機能を大幅に拡張するため\\nのシンプルで強力なアプローチについても解説しま\\nす。' metadata={'source': './DataStax_Vector-Search.pdf', 'page': 0}\n",
            "page_content='20. Karpathy, Andrej. “Andrej Karpathy on Twitter: \"The hottest new programming language is\\nEnglish.\"” Twitter , 2023年 1月24日 ,https://twitter .com/karpathy/status/1617979122625712128\\n21. “MTEB Leaderboard - a Hugging Face Space by mteb.” Hugging Face,\\nhttps://huggingface.co/spaces/mteb/leaderboard\\n22.https://platform.openai.com/docs/guides/embeddings/types-of-embedding-models\\n23. Instructor Text Embedding, https://instructor-embedding.github.io/\\n24. “Libraries - OpenAI API.” Platform OpenAI, https://platform.openai.com/docs/libraries\\n25. “Vertex AI.” Google Cloud, https://cloud.google.com/vertex-ai\\n©2023 DataStax Inc.、全著作権所有。 DataStax は、米国およびその他の国における DataStax, Inc. およびそ\\nの子会社の登録商標です。\\n24' metadata={'source': './DataStax_Vector-Search.pdf', 'page': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ベクトルストアの構築"
      ],
      "metadata": {
        "id": "lWMh_qez4mGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/docs/integrations/vectorstores/cassandra"
      ],
      "metadata": {
        "id": "iVdDamz_dQ2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_KEYSPACE = 'langchain'\n",
        "session.set_keyspace(YOUR_KEYSPACE)\n",
        "session"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im1ixFaIJ9Mw",
        "outputId": "195f97ae-4559-474d-f6aa-cc4517071562"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cassandra.cluster.Session at 0x786fccc1c640>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_function = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "IeNgg0f1dLkn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_name = \"pdf\"\n",
        "\n",
        "docsearch = Cassandra.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_function,\n",
        "    session=session,\n",
        "    keyspace=YOUR_KEYSPACE,\n",
        "    table_name=table_name,\n",
        ")"
      ],
      "metadata": {
        "id": "DatB7b8pdYzV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の操作により、下記のテーブルが作成されます。\n",
        "```\n",
        "CREATE TABLE langchain.pdf (\n",
        "    row_id text PRIMARY KEY,\n",
        "    attributes_blob text,\n",
        "    body_blob text,\n",
        "    metadata_s map<text, text>,\n",
        "    vector vector<float, 1536>\n",
        ") WITH additional_write_policy = '99p'\n",
        "    AND bloom_filter_fp_chance = 0.01\n",
        "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
        "    AND comment = ''\n",
        "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy'}\n",
        "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
        "    AND crc_check_chance = 1.0\n",
        "    AND default_time_to_live = 0\n",
        "    AND gc_grace_seconds = 864000\n",
        "    AND max_index_interval = 2048\n",
        "    AND memtable_flush_period_in_ms = 0\n",
        "    AND min_index_interval = 128\n",
        "    AND read_repair = 'BLOCKING'\n",
        "    AND speculative_retry = '99p';\n",
        "CREATE CUSTOM INDEX eidx_metadata_s_pdf ON langchain.pdf (entries(metadata_s)) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\n",
        "CREATE CUSTOM INDEX idx_vector_pdf ON langchain.pdf (vector) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\n",
        "```\n",
        "\n",
        "Astra DBの「CQL Console」タブで、次のコマンドを実行して上記のDDLを確認することができます。\n",
        "\n",
        "```\n",
        "desc langchain.pdf;\n",
        "```\n",
        "\n",
        "下記のように、登録されたデータの件数を確認することができます。\n",
        "\n",
        "```\n",
        "token@cqlsh:langchain> select count(row_id) from langchain.pdf;\n",
        "\n",
        " system.count(row_id)\n",
        "----------------------\n",
        "                   24\n",
        "\n",
        "(1 rows)\n",
        "```"
      ],
      "metadata": {
        "id": "RyS0PtoMu99M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記で、一度作成済みのテーブルを利用する場合には、引数中の`documents=docs`を省略して、以下のように初期化できます。\n",
        "\n",
        "```\n",
        "from langchain.vectorstores import Cassandra\n",
        "docsearch = Cassandra(\n",
        "  embedding=embedding_function,\n",
        "  session=session,\n",
        "  keyspace=YOUR_KEYSPACE,\n",
        "  table_name=\"pdf\",\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "j2SZwxf4v1r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Cassandra\n",
        "docsearch = Cassandra(\n",
        "  embedding=embedding_function,\n",
        "  session=session,\n",
        "  keyspace=YOUR_KEYSPACE,\n",
        "  table_name=\"pdf\",\n",
        ")"
      ],
      "metadata": {
        "id": "mraUEgHN419j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"RAGのユースケース\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "print(len(docs))\n",
        "for doc in docs:\n",
        "  print(doc)"
      ],
      "metadata": {
        "id": "yK39q3wldnBj",
        "outputId": "c9fec3b6-e7e4-4449-8501-d4fe19260bb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "page_content='RAG AIアプリケーションの実装\\n検索拡張生成（ RAG）を用いた生成 AIアプリを構築するための、基本的なフロー（図 3）とアーキテクチャダイアグラ\\nム（図 3.1）を紹介しました。ここでは、それらをさらに詳しく見てみましょう。\\nRAGシステムを構築するには、次の 2つのワークフローが必要です。\\n●ナレッジベースの作成\\n●生成プロセスの実行\\n以下、それぞれについて見ていきます。\\nナレッジベースの作成\\nまず、 RAGアプリケーションがアクセスする、エンベディングデータからなるナレッジベースの構築について見てみま\\nしょう。このワークフローには複数のステップがあり、その概要を図 4に示します。\\n図4.データセットをベクトルストアに変換するワークフロー\\n1.プロプライエタリデータの特定\\n初めのステップは、どのようなアプリケーションを構築しているかによって異なってきます。まず、アプリケー\\nションで使用できそうなデータセットを検討することになりますが、その際には、そのデータソースはどのよう\\nな固有の情報を提供するか？、エンベディングに利用できる公開データセットはあるか？、といった点を検\\n討する必要があります。\\n公開データセットの中には、 LLM構築時のトレーニングに用いられたデータセットよりも新しいデータが含ま\\nれている可能性があり、新たに、そうしたデータセットを用いることも考えられます。たとえば、最近のニュー\\nス記事や最新の開発者用ドキュメントといったものがあります。\\n2.エンベディングモデルの選択\\nエンベディングモデルの選択において、ナレッジベース用のエンベディングモデルと最終的な応答のテキス\\n10' metadata={'page': '9.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='RAG AIアプリケーションの実装\\n検索拡張生成（ RAG）を用いた生成 AIアプリを構築するための、基本的なフロー（図 3）とアーキテクチャダイアグラ\\nム（図 3.1）を紹介しました。ここでは、それらをさらに詳しく見てみましょう。\\nRAGシステムを構築するには、次の 2つのワークフローが必要です。\\n●ナレッジベースの作成\\n●生成プロセスの実行\\n以下、それぞれについて見ていきます。\\nナレッジベースの作成\\nまず、 RAGアプリケーションがアクセスする、エンベディングデータからなるナレッジベースの構築について見てみま\\nしょう。このワークフローには複数のステップがあり、その概要を図 4に示します。\\n図4.データセットをベクトルストアに変換するワークフロー\\n1.プロプライエタリデータの特定\\n初めのステップは、どのようなアプリケーションを構築しているかによって異なってきます。まず、アプリケー\\nションで使用できそうなデータセットを検討することになりますが、その際には、そのデータソースはどのよう\\nな固有の情報を提供するか？、エンベディングに利用できる公開データセットはあるか？、といった点を検\\n討する必要があります。\\n公開データセットの中には、 LLM構築時のトレーニングに用いられたデータセットよりも新しいデータが含ま\\nれている可能性があり、新たに、そうしたデータセットを用いることも考えられます。たとえば、最近のニュー\\nス記事や最新の開発者用ドキュメントといったものがあります。\\n2.エンベディングモデルの選択\\nエンベディングモデルの選択において、ナレッジベース用のエンベディングモデルと最終的な応答のテキス\\n10' metadata={'page': '9.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='最後に、アプリは検索に該当した上位数件のデータのテキストを取得し、それらを LLMに与える命令文の作成に活\\n用して、 LLMにタスクの結果を生成させます。\\n図3.RAGパターンを用いた LLMアプリケーションの基本フロー\\nこのアプローチには、次の利点があります。\\n●ハルシネーションの軽減\\nAIの分野では、一見すると尤もらしく見えるものの、事実とは異なる、あるいは文脈に適さない回答が出力\\nされることを「ハルシネーション（ Hallucination：幻覚）」と呼びます。 RAGデザインパターンは、ベクトル検\\n索によって抽出されたドキュメントに焦点を当てるように LLMに指示して、応答を生成します。 LLMのト\\nレーニングに用いられたデータセットの全体ではなく、より文脈に適したドキュメントセットを使用することで、\\nクエリへの応答は、ユーザーの意図をより正確に汲み取ったものになります。\\n●プロプライエタリデータの活用\\n企業が LLMアプリケーションを開発する場合、 LLMの生成する応答を、その企業の製品 ・サービス とユース\\nケースに対応した内容にするために、そのアプリケーションの要件に関連した企業の保有する情報・ドキュ\\nメントを、 LLMに直接提供することができます。\\n●LLM「トレーニング」の簡便さ\\nRAGパターンは、特定のアプリケーションに合わせてモデルを調整する方法の中でも、実装および保守が\\n容易です。プロプライエタリデータへの新しいドキュメントの追加、既存のドキュメントの変更、または新しい\\nデータソースが利用可能になったときにも、エンベディングデータを更新して、モデルを最新の状態に、そし\\nてその応答を正確なものに保つことが容易です。\\n次の図 3.1は、 RAGを使用したアプリケーションまたはサービスを実行するための完全なアーキテクチャの概要を示\\nしています。このアーキテクチャでは、エージェントという新たな概念が導入されています。エージェントとは、自動化\\nされた推論・意思決定エンジンです。エージェントは、ユーザーからの入力すなわちクエリを受け取り、それに応じて\\n何を行うかを決定するアプリケーション・コンポーネントまたはプログラミング・コードです。\\n図3で紹介した単純な RAGの例では、エージェントのタスクは、ユーザークエリのエンベディング、ベクトル検索の実\\n行、検索結果を含む LLMのプロンプトの作成といった一連の基本的なプロセスからなっていました。\\n8' metadata={'page': '7.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='より複雑なアプリケーションでは、タスクを複数のサブタスクに分割した上で、問題を解決するために外部ツールを使\\n用する必要があるかどうかを判断したり、アプリケーション用のメモリを使ってデータを保存し維持するために、エー\\nジェントが利用される場合があります。\\n図3.1RAGパターンを用いたアーキテクチャのコンテキストビュー\\nここで、エンベディングデータの保存とベクトル検索の実行にベクトル対応データベースを採用すると、利用可能な\\nデータ量を非常に大規模なデータセットに拡張しながら、ドキュメント検索速度の性能を維持できるという利点を得る\\nことができます。ベクトル対応データベースは、あらゆる生成エクスペリエンスの基本要件である、類似性検索を高\\nい効率で実現する機能を備えています。\\nベクトル対応データベースの採用\\nSequoia Capital 社の最近の調査10によると、生成 AIに取り組む 88%の企業が、生成 AIアプリ\\nケーションの実現に向けて、ドキュメント検索メカニズムが、今後インフラストラクチャを構成する\\n重要な部分になると考えています。\\n9' metadata={'page': '8.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='とになります。それは例えば、 AI安全性分析、品質管理解析、そして LLMからの応答に対してアプリケー\\nションのユースケースに固有の付加的なフィルタリングやアジャストメントなどを組み込むというものです。\\n多くのシステムでは、この後処理のステージを、生成された応答を、後で集合的にレビューするために、そ\\nのコピーをシステムに保存する機会としても使用します。 LLM応答の後処理が完了すると、最終的な応答\\nがユーザーに返され、一連の RAGプロセスの全体が完了します。\\n16' metadata={'page': '15.0', 'source': './DataStax_Vector-Search.pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "Ba7nukJTeCnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\":1})"
      ],
      "metadata": {
        "id": "3Cf1V6q3eE-G"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.agents.agent_toolkits import create_retriever_tool, create_conversational_retrieval_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.callbacks import StreamlitCallbackHandler\n",
        "from langchain.schema import BaseRetriever, Document, SystemMessage"
      ],
      "metadata": {
        "id": "dgVtEjZEe1mI"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.verbose = True"
      ],
      "metadata": {
        "id": "CYtpk9q-53D9"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "        retriever, \"my_retrevier\", \"Useful when searching for the information about technologies\")\n",
        "tools=[retriever_tool]\n",
        "\n",
        "system_message_content = \"\"\"\n",
        "    Please answer using the given my_retreiver.\n",
        "    \"\"\"\n",
        "system_message_content = f\"{system_message_content} All the responses should be in Japanese language.\"\n",
        "system_message = SystemMessage(content=system_message_content)\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, system_message=system_message, verbose=True, max_tokens_limit=500)"
      ],
      "metadata": {
        "id": "xGysR-omAtVv"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"DATASTAXについて説明して\")"
      ],
      "metadata": {
        "id": "pbCG_xPsBGra",
        "outputId": "ca7f91e6-999f-454a-be21-446501274abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m DataStaxに関する情報を探す\n",
            "Action: my_retrevier\n",
            "Action Input: DataStax\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3m[Document(page_content='カテゴリー情報を利用することができます。\\n4.エンベディングデータの生成と保存\\n先に、エンベディングに用いるためにモデルを選択しました。このモデルを用いてベクトルを生成することに\\nなります。\\nこのプロセスの最初のステップは、テキストのチャンク化です。これは、テキストをより短い文字列に分割す\\nる方法を決定するプロセスです。これは主に次の２つの理由から行われます。\\nまず、エンベディングモデルには入力として受け入れることのできるテキストの量に制限があるためです。\\ntext-ada-002 の場合、その制限は 8,192トークンです。受け入れられる文字列の長さがより短いモデルも\\nあります。\\nまた、チャンク化要否の判断は、モデルが持つ制限以外にも、データソースとアプリケーションの両方の面\\nからも検討する必要があります。アプリケーションがドキュメント全体をユーザーに返す必要がある場合、モ\\nデルが許す限り、できるだけ大きなチャンクを用いることは合理的です。あるいは、アプリケーションが、例\\nえば質疑応答システムの拡張といった場合のように、様々な事実に依存している場合、ドキュメント全体よ\\nりも、その中の数センテンス程度の短いチャンクの方が、一つのトピックに関する事実やアイデアを扱って\\nいる可能性が高く、このユースケースにより適しています。\\nさて、チャンクの長さが決まったら、次はチャンク化に関する戦略を立てる必要があります。これには多くの\\n方法があります。\\n最も簡単な方法は、テキストを均等な長さのチャンクに分割することです。この場合、重要な事実を説明し\\nている途中でチャンクが単語や文を分割してしまうことが生じる可能性があるため、チャンクのオーバーラッ\\nプを許容すると文書内の概念を適切に捉えるのに役立ちます。\\nより高度な、広く用いられている戦略として、ドキュメントの、または言語の構造に基づいたチャンク化があ\\nります。例としては、ドキュメントのセクション、段落ごと、またはセンテンスの境界を基準にしたチャンク化が\\n挙げられます。\\nドキュメントのチャンクのセットを作成したら、各チャンクのベクトルを、先の手順で定義した関連メタデータと\\n共に、データストアに保存します。これにより、近似最近傍探索（ ANN: Approximate Nearest Neighbor\\n）を利用した高速ベクトル検索が可能になります。 DataStaxの Apache Cassandraマネージドサービスで\\nあるAstraDB は、ベクトル検索の機能を有しており、大規模なベクトルデータセットを操作する場合の優れ\\nた選択肢となります。そしてまた、 AstraDBのベクトル検索機能は、高い可用性と柔軟なスケールを有した\\n分散データベース Apache Cassandraの拡張機能であるため、ベクトルとともに、そのベクトルに関連する\\nメタデータを保存しておくことも容易であり、ベクトル検索のみではなく、メタデータを使った操作との組み合\\nわせについても、利点を持っています。このような大規模データ利用のための信頼性・スケールと、データ\\nベースとしての汎用性は、 AstraDBの持つ、単なるベクトルデータベースとは異なる利点です。\\n12', metadata={'page': '11.0', 'source': './DataStax_Vector-Search.pdf'})]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m DataStaxとは、Apache CassandraマネージドサービスであるAstraDBを利用して、近似最近傍探索（ANN）を利用した高速ベクトル検索を行えるサービスであることがわかった。\n",
            "Final Answer: DataStaxは、Apache CassandraマネージドサービスであるAstraDBを利用して、近似最近傍探索（ANN）を利用した高速ベクトル検索を提供しているサービスです。\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DataStaxは、Apache CassandraマネージドサービスであるAstraDBを利用して、近似最近傍探索（ANN）を利用した高速ベクトル検索を提供しているサービスです。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## チェーンタイプごとの`RetriavalQA`\n",
        "\n",
        "### `stuf`\n",
        "\n",
        " - Top-K:2"
      ],
      "metadata": {
        "id": "ZxX_ugPP2_ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\":2})\n",
        "llm = ChatOpenAI(temperature=0, streaming=True)\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        #chain_type=\"map_reduce\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        verbose=True\n",
        "    )"
      ],
      "metadata": {
        "id": "XmROX2B16VKN"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"DATASTAXについて説明して\"\n",
        "response = qa(query)"
      ],
      "metadata": {
        "id": "5Xojob1b-v8x",
        "outputId": "e8e06192-ac91-4638-fdea-073c1b1247cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "cvfdcLFKFW3q",
        "outputId": "4ebbdb42-3feb-4d33-8fcf-3593cc2c22ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'DATASTAXについて説明して', 'result': 'DataStaxは、分散データベース管理システムを提供する企業です。DataStaxの主力製品は、Apache Cassandraをベースにしたデータベース管理システムであり、大規模なデータセットを処理するための高い可用性とスケーラビリティを提供します。\\n\\nDataStaxのデータベース管理システムは、分散データベースの特徴であるデータの冗長性と自動的なデータの分散を活用して、高い可用性と耐久性を実現します。また、データの読み書き操作を並列に処理することで、高速なデータアクセスを実現します。\\n\\nDataStaxのデータベース管理システムは、さまざまな業界や用途で利用されています。例えば、金融機関では取引データの管理やリアルタイム分析に使用され、オンライン小売業者では顧客データの管理や在庫管理に使用されています。\\n\\nまた、DataStaxはベクトル対応データベースも提供しており、ベクトルデータの保存と高速なベクトル検索を実現します。これにより、生成AIや機械学習モデルの応用において、類似性検索やパターンマッチングなどの高度な処理が可能となります。\\n\\nDataStaxの製品は、クラウド上での利用にも対応しており、DataStax AstraDBというマネージドサービスを提供しています。これにより、データベースのセットアップや管理を簡単に行うことができます。\\n\\n総括すると、DataStaxは分散データベース管理システムを提供する企業であり、高い可用性とスケーラビリティを備えたデータベースを提供しています。また、ベクトル対応データベースも提供しており、生成AIや機械学習モデルの応用において高速なベクトル検索を実現します。', 'source_documents': [Document(page_content='カテゴリー情報を利用することができます。\\n4.エンベディングデータの生成と保存\\n先に、エンベディングに用いるためにモデルを選択しました。このモデルを用いてベクトルを生成することに\\nなります。\\nこのプロセスの最初のステップは、テキストのチャンク化です。これは、テキストをより短い文字列に分割す\\nる方法を決定するプロセスです。これは主に次の２つの理由から行われます。\\nまず、エンベディングモデルには入力として受け入れることのできるテキストの量に制限があるためです。\\ntext-ada-002 の場合、その制限は 8,192トークンです。受け入れられる文字列の長さがより短いモデルも\\nあります。\\nまた、チャンク化要否の判断は、モデルが持つ制限以外にも、データソースとアプリケーションの両方の面\\nからも検討する必要があります。アプリケーションがドキュメント全体をユーザーに返す必要がある場合、モ\\nデルが許す限り、できるだけ大きなチャンクを用いることは合理的です。あるいは、アプリケーションが、例\\nえば質疑応答システムの拡張といった場合のように、様々な事実に依存している場合、ドキュメント全体よ\\nりも、その中の数センテンス程度の短いチャンクの方が、一つのトピックに関する事実やアイデアを扱って\\nいる可能性が高く、このユースケースにより適しています。\\nさて、チャンクの長さが決まったら、次はチャンク化に関する戦略を立てる必要があります。これには多くの\\n方法があります。\\n最も簡単な方法は、テキストを均等な長さのチャンクに分割することです。この場合、重要な事実を説明し\\nている途中でチャンクが単語や文を分割してしまうことが生じる可能性があるため、チャンクのオーバーラッ\\nプを許容すると文書内の概念を適切に捉えるのに役立ちます。\\nより高度な、広く用いられている戦略として、ドキュメントの、または言語の構造に基づいたチャンク化があ\\nります。例としては、ドキュメントのセクション、段落ごと、またはセンテンスの境界を基準にしたチャンク化が\\n挙げられます。\\nドキュメントのチャンクのセットを作成したら、各チャンクのベクトルを、先の手順で定義した関連メタデータと\\n共に、データストアに保存します。これにより、近似最近傍探索（ ANN: Approximate Nearest Neighbor\\n）を利用した高速ベクトル検索が可能になります。 DataStaxの Apache Cassandraマネージドサービスで\\nあるAstraDB は、ベクトル検索の機能を有しており、大規模なベクトルデータセットを操作する場合の優れ\\nた選択肢となります。そしてまた、 AstraDBのベクトル検索機能は、高い可用性と柔軟なスケールを有した\\n分散データベース Apache Cassandraの拡張機能であるため、ベクトルとともに、そのベクトルに関連する\\nメタデータを保存しておくことも容易であり、ベクトル検索のみではなく、メタデータを使った操作との組み合\\nわせについても、利点を持っています。このような大規模データ利用のための信頼性・スケールと、データ\\nベースとしての汎用性は、 AstraDBの持つ、単なるベクトルデータベースとは異なる利点です。\\n12', metadata={'page': '11.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='より複雑なアプリケーションでは、タスクを複数のサブタスクに分割した上で、問題を解決するために外部ツールを使\\n用する必要があるかどうかを判断したり、アプリケーション用のメモリを使ってデータを保存し維持するために、エー\\nジェントが利用される場合があります。\\n図3.1RAGパターンを用いたアーキテクチャのコンテキストビュー\\nここで、エンベディングデータの保存とベクトル検索の実行にベクトル対応データベースを採用すると、利用可能な\\nデータ量を非常に大規模なデータセットに拡張しながら、ドキュメント検索速度の性能を維持できるという利点を得る\\nことができます。ベクトル対応データベースは、あらゆる生成エクスペリエンスの基本要件である、類似性検索を高\\nい効率で実現する機能を備えています。\\nベクトル対応データベースの採用\\nSequoia Capital 社の最近の調査10によると、生成 AIに取り組む 88%の企業が、生成 AIアプリ\\nケーションの実現に向けて、ドキュメント検索メカニズムが、今後インフラストラクチャを構成する\\n重要な部分になると考えています。\\n9', metadata={'page': '8.0', 'source': './DataStax_Vector-Search.pdf'})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Top-K:5\n",
        "\n",
        "ベクトルストアから取得した上位５件をを全て一度のプロンプトで利用する。トークン数の上限によってエラー発生。"
      ],
      "metadata": {
        "id": "tPEZx9O9F85L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\":5})\n",
        "llm = ChatOpenAI(temperature=0, streaming=True)\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        #chain_type=\"map_reduce\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        verbose=True\n",
        "    )\n",
        "query = \"DATASTAXについて説明して\"\n",
        "response = qa(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "l4RI9jWJFcCa",
        "outputId": "a8b99563-54ab-4b43-9f4a-f3ac28d209ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-482bbb7765b3>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DATASTAXについて説明して\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs = (\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         answer = self.combine_documents_chain.run(\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0minput_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    511\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs = (\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/stuff.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# Call predict on the LLM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     async def acombine_docs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"funny\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs = (\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    458\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         flattened_outputs = [\n\u001b[1;32m    351\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 results.append(\n\u001b[0;32m--> 339\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    340\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 )\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                 return self._generate(\n\u001b[0m\u001b[1;32m    493\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_message_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36m_generate_from_stream\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_generate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatGenerationChunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mChatResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgeneration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatGenerationChunk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgeneration\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mgeneration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mdefault_chunk_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAIMessageChunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         for chunk in self.completion_with_retry(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         )\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 842\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    843\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;31m# to completion before attempting to access the response text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 5524 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\":5})\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"map_reduce\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        verbose=True\n",
        "    )\n",
        "query = \"DATASTAXについて説明して\"\n",
        "response = qa(query)"
      ],
      "metadata": {
        "id": "8sXUFZNF3X5q",
        "outputId": "7baf98c9-257a-4267-f6ab-a4f92032f295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\":5})\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"map_rerank\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        verbose=True\n",
        "    )\n",
        "query = \"'DATASTAX'について説明して\"\n",
        "response = qa(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "L8xgLNiL3ew_",
        "outputId": "5f627a60-02db-44ac-d6eb-0e07376a6f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:349: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-9f46e15b3ec8>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"'DATASTAX'について説明して\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs = (\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         answer = self.combine_documents_chain.run(\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0minput_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    511\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs = (\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/map_rerank.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0melement\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mother\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         results = self.llm_chain.apply_and_parse(\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;31m# FYI - this is parallelized and so it is fast.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_variable_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mapply_and_parse\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    352\u001b[0m         )\n\u001b[1;32m    353\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     def _parse_generation(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_parse_generation\u001b[0;34m(self, generation)\u001b[0m\n\u001b[1;32m    358\u001b[0m     ) -> Sequence[Union[str, List[str], Dict[str, str]]]:\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    361\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             return [\n\u001b[0;32m--> 361\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/regex.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_output_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not parse output: {text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 return {\n",
            "\u001b[0;31mValueError\u001b[0m: Could not parse output: I don't know."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "9z9kjA2LHAvk",
        "outputId": "bfa7cbad-2863-4515-fff7-7d891d007a1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'DATASTAXについて説明して', 'result': 'This document does not answer the question', 'source_documents': [Document(page_content='カテゴリー情報を利用することができます。\\n4.エンベディングデータの生成と保存\\n先に、エンベディングに用いるためにモデルを選択しました。このモデルを用いてベクトルを生成することに\\nなります。\\nこのプロセスの最初のステップは、テキストのチャンク化です。これは、テキストをより短い文字列に分割す\\nる方法を決定するプロセスです。これは主に次の２つの理由から行われます。\\nまず、エンベディングモデルには入力として受け入れることのできるテキストの量に制限があるためです。\\ntext-ada-002 の場合、その制限は 8,192トークンです。受け入れられる文字列の長さがより短いモデルも\\nあります。\\nまた、チャンク化要否の判断は、モデルが持つ制限以外にも、データソースとアプリケーションの両方の面\\nからも検討する必要があります。アプリケーションがドキュメント全体をユーザーに返す必要がある場合、モ\\nデルが許す限り、できるだけ大きなチャンクを用いることは合理的です。あるいは、アプリケーションが、例\\nえば質疑応答システムの拡張といった場合のように、様々な事実に依存している場合、ドキュメント全体よ\\nりも、その中の数センテンス程度の短いチャンクの方が、一つのトピックに関する事実やアイデアを扱って\\nいる可能性が高く、このユースケースにより適しています。\\nさて、チャンクの長さが決まったら、次はチャンク化に関する戦略を立てる必要があります。これには多くの\\n方法があります。\\n最も簡単な方法は、テキストを均等な長さのチャンクに分割することです。この場合、重要な事実を説明し\\nている途中でチャンクが単語や文を分割してしまうことが生じる可能性があるため、チャンクのオーバーラッ\\nプを許容すると文書内の概念を適切に捉えるのに役立ちます。\\nより高度な、広く用いられている戦略として、ドキュメントの、または言語の構造に基づいたチャンク化があ\\nります。例としては、ドキュメントのセクション、段落ごと、またはセンテンスの境界を基準にしたチャンク化が\\n挙げられます。\\nドキュメントのチャンクのセットを作成したら、各チャンクのベクトルを、先の手順で定義した関連メタデータと\\n共に、データストアに保存します。これにより、近似最近傍探索（ ANN: Approximate Nearest Neighbor\\n）を利用した高速ベクトル検索が可能になります。 DataStaxの Apache Cassandraマネージドサービスで\\nあるAstraDB は、ベクトル検索の機能を有しており、大規模なベクトルデータセットを操作する場合の優れ\\nた選択肢となります。そしてまた、 AstraDBのベクトル検索機能は、高い可用性と柔軟なスケールを有した\\n分散データベース Apache Cassandraの拡張機能であるため、ベクトルとともに、そのベクトルに関連する\\nメタデータを保存しておくことも容易であり、ベクトル検索のみではなく、メタデータを使った操作との組み合\\nわせについても、利点を持っています。このような大規模データ利用のための信頼性・スケールと、データ\\nベースとしての汎用性は、 AstraDBの持つ、単なるベクトルデータベースとは異なる利点です。\\n12', metadata={'page': '11.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='より複雑なアプリケーションでは、タスクを複数のサブタスクに分割した上で、問題を解決するために外部ツールを使\\n用する必要があるかどうかを判断したり、アプリケーション用のメモリを使ってデータを保存し維持するために、エー\\nジェントが利用される場合があります。\\n図3.1RAGパターンを用いたアーキテクチャのコンテキストビュー\\nここで、エンベディングデータの保存とベクトル検索の実行にベクトル対応データベースを採用すると、利用可能な\\nデータ量を非常に大規模なデータセットに拡張しながら、ドキュメント検索速度の性能を維持できるという利点を得る\\nことができます。ベクトル対応データベースは、あらゆる生成エクスペリエンスの基本要件である、類似性検索を高\\nい効率で実現する機能を備えています。\\nベクトル対応データベースの採用\\nSequoia Capital 社の最近の調査10によると、生成 AIに取り組む 88%の企業が、生成 AIアプリ\\nケーションの実現に向けて、ドキュメント検索メカニズムが、今後インフラストラクチャを構成する\\n重要な部分になると考えています。\\n9', metadata={'page': '8.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='それを理解するためだけに別のモデルを構築する必要さえあります。生データから、課題に関連するデー\\nタを特徴量として選別することは、多くの場合、困難な問題であることが分かっています。たとえば、 Netﬂix\\nの最も重要な目標の 1つとして、ユーザーが好むであろうコンテンツをユーザーのログイン時に予測するこ\\nとがありますが、このケースにおいても、特徴量は多くのデータセットにまたがっており、複雑を極めていま\\nす。\\n2.特徴量を使った要件に応じた機械学習モデルのトレーニング\\n画像分類などの一般的な機械学習の問題であれば、個別の問題に合わせて「ファインチューニング」する\\nことできる既製のモデルが存在する場合があります。 GoogleのAutoML Vision19はそのようなシステム\\nの好例です。既存のモデルに、犬がどのようなものであるかを示すサンプルデータセットを提供するだけ\\nで、画像に写った犬を認識することのできるチューニングされたバージョンのモデルが得られます。一方、\\nファインチューニングするための既存のモデルを見つけることができないようなタイプの問題があります。そ\\nの場合、独自のモデルをトレーニングする必要がありますが、これは多大な時間を要し、計算コストが跳ね\\n上がる傾向があります。一方、新しいユースケースに合わせて既存のモデルを変更する場面において、求\\nめる結果を導き出すためには、基礎となる数学に対する深い理解が必要になります。\\n3.モデルをデプロイし、 APIとして公開\\n機械学習モデルをデプロイする際、アプリケーションが機械学習モデルを提供するサービスを利用できるよ\\nうにすることになります。それには、モデルサービスが必要なデータに確実にアクセスできること、アプリ\\nケーションがモデルサービスにアクセスし予測ステップの結果をリクエストするためのインターフェースを設\\n計することが必要です。このステップの API実装の選択は、予測がどのように使用されるかに直接的に依\\n存します。また、認証 /認可についての配慮と実装も必要になります。\\n4.プロダクション環境でモデルを実行\\nモデルがプロダクションで利用され始めると、しばしば新たな課題に遭遇します。一般的な例としては、モデ\\nルの予測（出力）ステップのレイテンシ、時の経過に伴うモデルの陳腐化によるアウトプットの品質低下、モ\\nデルにとって重要なデータの投入までに生じる時間的ギャップなどが挙げられます。これらの問題のそれぞ\\nれを解決するために利用できるソリューションはありますが、ピーク性能を維持し、時間の経過とともに変化\\nするデータや、データ量に対処するには、システムの長期運用ライフサイクルの様々な側面に繊細な注意\\nを払い続ける必要があります。\\nモデルのデプロイと公開には一見特に複雑なところはないと思われるかもしれませんが、課題がないわけではあり\\nません。また、上記のプロセスのその他すべてのステップに渡って、しばしばオーダーメイドのソリューションが必要\\nになります。さらに、従来のスタイルの AIアプリケーションを構築して出荷するために必要なスキルセットを持つチー\\nムを編成・維持するのは決して容易ではありません。達成するには、およそ３、４人からなるチームが必要になりま\\nす。たとえば、従来の AIアプリケーションチームは、それぞれスキルの異なる、次のような役割で構成されるのが\\n一般的です。\\n●データエンジニア -使用する生のデータセットを取得し、探索と利用を容易にするためにデータを保存およ\\nびクリーニングする責任を持ちます。データエンジニアチームは、データモデリング、 ETLパイプライン、\\nデータウェアハウジングの専門家集団です。\\n●データサイエンティスト -データを機械学習モデルで活用できる ようにベクトルに変換する、データ探索と特\\n徴量エンジニアリングを担当します。このチームは、モデルのソリューション化に際して、バイアスや過学習\\n3', metadata={'page': '2.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='サービスを提供する SaaS企業）は、 Payments、 Checkouts、 Billingなどの製品を提供しています。ご覧の通り、製\\n品名には一般名詞が使用されています。このような場合、 LLMにとって、固有名詞としての製品名と、一般名詞とし\\nての会計用語とを区別することは難しいものになります。\\nここで、「 My stripe payments failed.」というフレーズを考えてみましょう。\\nユーザーは、「 Stripe Paymentsをセットアップしようとしたが、うまくいかなかった」と言っているのかもしれません\\nし、仔細に表現すると「 Stripeからの請求書を支払おうとしたが、失敗した」という内容を伝えようとしていたのかもし\\nれません。\\nこの単純な例では、フレーズを「 Stripe Payments」という製品に関する議論として分類すべきかどうかを判断するこ\\nとは困難に見えます。\\n対策として、文書内で Stripe paymentsという製品について説明しているまとまった量のサンプル（正事象）と、\\nStripeという単語の近くに「 payments」という単語が出現しているが、製品の名称としてではなく会計用語として使\\n用されている同規模のサンプル（反事象）からなる、十分な量のサンプルで LLMをファインチューニングすることが考\\nえられます。これによって、 LLMは、これら 2つのケースを区別できるようになる可能性があります。\\nサマリー :\\n●メリット -分類器の精度向上\\n●デメリット -少なくとも数百のトレーニングサンプルのセットを維持する必要性\\n●デメリット -クラウドベンダーの提供するファインチューニングモデルは標準の LLMより高額\\nエンベディングによる分類\\n分類の３番目のアプローチでは、エンベディングを使用して、ロジスティック回帰、ランダムフォレスト、サポートベク\\nターマシン (SVM)などの従来の分類モデルをトレーニングします。\\nこのアプローチでは、分類器に与えるラベルごとに大量のエンベディングデータが準備可能なトレーニングデータ\\nセットを用います。そして、 Scikit-Learn やLightGBM などの標準ライブラリを使用して分類器をトレーニングしま\\nす。\\nこのアプローチは、エンベディングと大規模なトレーニングデータセットに基づいており、テキストデータの複雑な分\\n類問題で非常にうまく機能することが期待されます。\\nサマリー :\\n●メリット -分類器の精度向上 (ただし、必ずしも LLMアプローチよりも優れているとは限らない )\\n●メリット -適用されるラベルあたりのコストが非常に低い。ベンダーの LLMは、処理されたトークンによって\\n課金されるため、文書が長大、あるいは分類文書セットの数が非常に大規模、またはその両方である場\\n合、LLM分類器の実行コストは非常に高くなる可能性があります\\n●デメリット -トレーニングデータセットの構築と維持にかかる非常に多くの時間と労力\\nLLMキャッシング\\nLLMの APIコールにはコストがかかり、低速であることもよくあります。多くのシナリオで（例えば、エンタープライズ検\\n索）質問や検索クエリの大多数が、意味的には同じであるのに、単に同じ質問に対して表現が若干異なっているだ\\n21', metadata={'page': '20.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='5.エンベディングデータの更新\\n企業が保有するデータは、時間の経過とともに変化していきます。ハルシネーションを避けるためには、新\\nしいデータソースが利用可能になったらすぐに、エンベディングデータを更新することが重要です。\\nデータソースの変更が発生するのは、ある時は、新しいドキュメントが利用可能になったためかもしれませ\\nんし、またある時は、過去にエンコード済みのコンテンツが更新されたためである場合もあります。\\n既存のドキュメントのエンベディングデータを更新するときは、企業が保有するデータの１オブジェクトに対し\\nて複数のエンベディングが登録されている可能性があることに注意する必要があります。データソースがテ\\nキストである場合、これはテキスト全体の長さと選択したチャンク化の戦略によって異なります。エンベディ\\nングデータを更新するときは、更新時にそのドキュメントから派生した、すべてのエンベディングデータを置\\nき換えるように注意する必要があります。\\nまた、すべてのエンベディングモデルの変換ロジックが決定論的であるわけではないことを覚えておくこと\\nは有益です。モデルによっては、同じテキストをエンコードする都度、異なる結果が得られる場合がありま\\nす。結果として得られるベクトルは通常、類似したものにはなるでしょうが、既存のエンベディングデータを\\n新しいエンベディングデータに置き換えるときには、置き換えるべき既存のエンベディングデータの特定の\\nために、エンベディングベクトルの生の値を用いることは得策ではありません。代わりに、一意の識別子を\\n使うことが考えられます。\\n生成プロセスの実行\\nここからは、前のセクションでエンコードしたデータセットを使用して LLMからの応答を生成する方法を詳細に見て\\nいきます。\\n図5.クエリから応答を生成するワークフロー\\n13', metadata={'page': '12.0', 'source': './DataStax_Vector-Search.pdf'})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger('openai').setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "mvXIdlpS4Wky"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"DATASTAXについて説明して\"\n",
        "response = qa(query)"
      ],
      "metadata": {
        "id": "On7ACijB3zQi",
        "outputId": "1dbcd9f0-f896-49d2-f8d9-72319764dfc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:349: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-acffd3a10f73>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DATASTAXについて説明して\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs = (\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         answer = self.combine_documents_chain.run(\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0minput_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    511\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs = (\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/map_rerank.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0melement\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mother\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         results = self.llm_chain.apply_and_parse(\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;31m# FYI - this is parallelized and so it is fast.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_variable_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mapply_and_parse\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    352\u001b[0m         )\n\u001b[1;32m    353\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     def _parse_generation(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_parse_generation\u001b[0;34m(self, generation)\u001b[0m\n\u001b[1;32m    358\u001b[0m     ) -> Sequence[Union[str, List[str], Dict[str, str]]]:\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    361\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             return [\n\u001b[0;32m--> 361\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/regex.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_output_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not parse output: {text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 return {\n",
            "\u001b[0;31mValueError\u001b[0m: Could not parse output: I don't know."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger('openai').setLevel(logging.WARN)\n",
        "langchain.verbose = False"
      ],
      "metadata": {
        "id": "0rQBzC7C4wWq"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        #chain_type=\"stuff\",\n",
        "        chain_type=\"map_rerank\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        verbose=True\n",
        "    )"
      ],
      "metadata": {
        "id": "3fahbIam3lqK"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"ベクトル検索について説明して\"\n",
        "response = qa(query)\n",
        "print(\"回答：\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Ki4wMwV84ouo",
        "outputId": "03c9496f-44b5-48da-8568-c6d4daba1f3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:349: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "回答：\n",
            "{'query': 'ベクトル検索について説明して', 'result': 'ベクトル検索は、ベクトル演算を使用してクエリに最も類似するデータを検索する方法です。クエリの表現をベクトル化し、そのベクトルを使用してデータのベクトルとの類似性を計算します。類似性に基づいてデータを検索することで、効率的な検索が実現されます。ベクトル検索は、セマンティック情報を保持するベクトル演算の一例であり、意味概念を捉えることが期待されます。', 'source_documents': [Document(page_content='け、ということが見られます。このような場合に、ベクトル検索を活用することにより、クエリを意味レベルで把握し、そ\\nの内容に応じてキャッシュから結果を検索することができます。\\nLLMキャッシュのシンプルな実装には、次の手順が必要です。\\n1. LLM に送信されたクエリごとに、エンベディングを生成します\\n2.クエリに対する LLMの応答を保存します\\n3.両方のエンベディングをベクターデータストアに保存します\\n4. LLM を呼び出す前に、ベクトル検索を使用して同様のクエリが以前にすでに実行されキャッシュされていな\\nいかを確認し、存在する場合はキャッシュされた結果を返します\\nこの手法により、本番アプリケーションで LLMを実行する際のコストとレイテンシの問題を大幅に削減できます。\\nただし、欠点の 1つとして、キャッシュされた応答に個人情報が保存され、別のユーザーの応答に利用されてしま\\nう、ということが起きないように細心の注意を払う必要があります。\\nエンベディングによる異常検出\\n異常（アノマリー）検出とは、特定のデータセット内で予想される動作、または通常の動作から大きく逸脱するパター\\nン、あるいはポイントを特定するプロセスを指します。例えば、典型的なパターンに従わない外れ値の検出や、通常\\nと異なる、あるいは非常に稀なイベントの検出、などがあります。\\n密度ベースの異常検出にベクトル検索を使用する簡単なプロセスは次のとおりです。\\n●アプリケーションが管理しているデータのエンベディングデータセットを作成\\n●アプリケーションへの入力（ドキュメントまたは画像）に対して、そのエンベディングデータを生成\\n●アプリケーションの既存のデータセットの中から入力されたエンベディングデータに最も近いものを検索\\n●入力データと検索されたデータとの間の距離が、ノーマルな範囲からの逸脱を示す、しきい値を超えている\\nかどうかを確認\\n●入力データを異常（アノマリー）としてラベル付け\\n22', metadata={'page': '21.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='この例では、ベクトルは 2次元の情報を持ち、ベクトルデータを構成するそれぞれの要素は、次の２つの情報を表し\\nてます： [<衣服の種類 >, <服の色>]。ここで、「 赤いシャツ」のベクトルは [1, 1]で、「青いズボン」のベクトルは\\n[-1, -1]で表されているとします。そこに、「緑色のスカート」を加えます。スカートは機能的にはズボンに近く、緑は色\\nのスペクトルとして青に近いため、そのエンベディングベクトルは「青いズボン」に近い位置にあるはずで、例えば\\n[-0.8, -0.8] のような値を取ると推測することができます。\\n上の例は、エンベディングを使ってベクトル空間を構築する方法と、その空間の中で、オブジェクトが存在する場所\\nについてのアイディアを示しています。\\n次に、エンベディングに対する算術演算がどのように機能するかを示す例を見てみましょう。予め注意しておくと、ベ\\nクトル空間は実際には非常に大きな次元を持つ（ベクトルは多くの数値で構成される）のが一般的です。ここで扱う\\nベクトル空間は、「人物の肩書き」と「性別」という二つの概念を捉えた情報を扱っています。図 2を参照ください。\\n6', metadata={'page': '5.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='図2.セマンティック情報を保持するベクトル演算の例\\n埋め込み（エンベディング）表現はベクトルであるため、ベクトル同士で算術演算を行うことができます。その際、その\\n演算結果のベクトルには、ベクトルの元となった自然言語の組み合わせに相当する意味概念が保持されることが期\\n待されます。\\nたとえば、 ベクトル同士の計算において、 《「王子」-「男性」+「女性」》 という計算式の結果として得\\nられるベクトルは、 「プリンセス」を表すベクトルと類似する値であると考えられます 。\\n同様に、《 「王子」-「男性」》 と《「プリンセス」 -「女性」》という２つの計算式の結果のベクトル\\nは、互いに非常に近い距離にあるでしょう。この場合、これらのベクトルは、「王または女王の直系の子孫である王\\n族（男女の別を問わない）」という概念を捉えていると見ることができます。\\n検索拡張生成（ RAG）パターン\\nLLMには、それが素のままで用いられた場合、生成された応答が特定のアプリケーションでは用をなさない可能性\\nがある、という限界があります。これは、 LLMに保存されている知識が通常そのモデルのトレーニングに用いられた\\nデータに限定されており、インターネット上で一般に入手できないような、最新のデータや、特定のドメインに関連す\\nるプロプライエタリデータが含まれていないことに由来します。\\n検索拡張生成（ RAG）の中心となるアイディアは、 LLMと、ユーザーが実行を依頼するタスクの解決に関連する独自\\nのドキュメントとを統合することです。\\n図3は、 RAGを用いたアプリケーションがどのように動作するかを示しています。まず、クエリがアプリケーションに対\\nしてサブミットされます。生成 AIアプリでは、クエリの表現は、検索エンジンタイプ、つまり「ジョージ・ワシントンの歴\\n史」というようなキーワード、または「ジョージ・ワシントンはいつ生まれたか」のような具体的な質問文、さらには\\n「ジョージ・ワシントンの生涯についてのエッセイを書け」という命令文、といったように様々な形式をとり得ます。独自\\nデータソースのエンベディングに使用したものと同じエンベディングモデルを使用してクエリのエンベディングを行い、\\nそのクエリのエンベディングを使用してベクトル検索を行うことによって、そのクエリに最も類似するデータを検索す\\nることができます。このようなデータ間の類似性に基づく効率的な検索は、ベクトル検索によって実現される重要な\\n機能です。\\n7', metadata={'page': '6.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='ホワイトペーパー\\n生成 AIアプリ\\nのための\\nベクトル検索\\nAIアプリケーション開発にベクトル検索を活用す\\nるための開発者/アーキテクト向けガイド\\nこのドキュメントは、生成 AIアプリケーションを企業独\\n自のデータと組み合わせて、設計・構築しようとしてい\\nる全ての方のためのガイドです。組織が理解すべき重\\n要な概念と考慮事項を取り扱うだけでなく、ベクトル検\\n索を用いて、 LLMの持つ機能を大幅に拡張するため\\nのシンプルで強力なアプローチについても解説しま\\nす。', metadata={'page': '0.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='しかし、 LLMに対して、いくら情報を与えたところで、それらが課題に関連する情報でない場合、役には立ちません。\\n意味的に課題に関連する情報（ semantically relevant information）を与えることが重要になります。ベクトル検索\\nを使えば、そのような情報を特定することは驚くほど簡単になります。\\nベクトル検索とは何か ?\\nベクトル検索は、類似した属性または特徴を持つ関連オブジェクトを検索する方法です。ベクトル検索を活用するこ\\nとができるオブジェクトの一般的な例には、テキスト、物理的な製品、画像、ビデオなどがあります。\\nベクトル検索では、エンベディングと呼ばれる特定のタイプの機械学習モデルを使用して、オブジェクトとそのコンテ\\nキストを記述します。エンベディングされたデータは、オブジェクトのセマンティクスを捉えるベクトルであり、次のセク\\nションで詳しく説明します。このアプローチにより、ベクトル検索では、正確にどの部分が類似する対象を探すことに\\nなるかについて前知識を持つ必要がなく、類似したデータを見つけることができます。\\n英語を例に挙げると、「 happy」、「 cheerful」、「 joyful」という単語はすべて同様の意味を持ちますが、従来のキー\\nワードベースの検索では、「 happy」というキーワードを用いた場合、「 cheerful」と「 joyful」に一致するドキュメントは\\n検索されません。これを解決するのが、ベクトル検索の能力です。ベクトル検索は意味を理解することができるた\\nめ、ユーザーが網羅的に指示せずとも、検索したい対象を伝えることができます。\\nもう一つの利点として、ベクトル検索は、近似最近傍 (ANN)検索と呼ばれるアルゴリズムのセットを使用して非常に\\n高速に実行できることがあります。このアプローチは非常に効率的であり、数学的に互いに類似するベクトルを素早\\nく導出します。\\nエンベディング（埋め込み）の基礎\\nエンベディング（埋め込み）は、テキストの意味を封じ込めたデータの数学的表現です。テキストをベクトル化して埋\\nめ込み（エンベディング）表現に変換するために、ドキュメントを構成する単語群から数値のリストへの変換を実施し\\nます。エンベディングの結果、ベクトル空間内では、互いに関連性の高い意味を持つベクトル同士は、近接した場所\\nに配置されることになります。\\nもっと分かりやすくするため、例を用いて考えてみましょう。図 1は、「衣服の種類」に関する次元と「服の色」という\\n属性に関する次元の 2つの次元を持つ単純なベクトル空間を示しています。\\n図1.衣服を表現する 2次元ベクトル空間\\n5', metadata={'page': '4.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='1.クエリのエンベディング\\nRAGアプリケーションにおけるテキスト生成はクエリから始まります。クエリはアプリケーションのエンド\\nユーザーから送信されたものを、そのまま利用する場合もあれば、それを元に LLMまたはその他の手続き\\n型コードによって新たに生成する場合もあります。\\n次に、クエリのテキストエンベディングを行って、クエリをベクトルにエンコードします。\\nここで使用されるエンベディングモデルが、ベクトル検索用のデータセットをエンベディングするために使用\\nされたものと同じモデルであることが重要です。そうでない場合、ベクトル検索は正しく機能しません。\\n2.ベクトル検索\\n次に、ベクトルデータストアからクエリに最も類似したエンベディングデータを取得することになります。ま\\nず、クエリベクトルの近くにあるベクトルを探すため、最近傍検索を行います。多くの場合、この検索は「上\\n位N件」の一致を求めることによって行われます。例えば、上位 3件の一致を利用する、というようなことが\\n一般に行われますが、この数は、エンベディングで使用したチャンクの長さ、コンテキストウィンドウ（ LLMに\\n許可される入力のサイズ）、およびユースケース等に応じて調整することになります。\\nベクトル検索を行う際に、検索結果のみでなく、クエリベクトルと検索結果ベクトルとの間の類似性について\\nのスコアまたは尺度を返すように要求することがあります。このスコアがしきい値を下回る場合、検索され\\nたドキュメントのクエリに対する一致度が十分なものでないとして、後続のプロセスで、この検索結果ドキュ\\nメントを、次のステップでは利用しないという決定を行うことができます。使用する実際のしきい値は、アプリ\\nケーション、データセット、および類似性メトリックによって異なります。スコアの範囲が、最大値を「 1」として\\n正規化されていると仮定する場合、カットオフの値として、「 0.5～0.8」の間から実験を開始するのが適切\\nと言えるでしょう。\\nベクトル検索により、たとえば数百、数千件規模の一致結果を取得すると共に、それぞれのスコアを得た\\n上で、エンベディングの生成に使用された元テキストを使って選別を行うことで、適切なカットオフを、ベクト\\nルデータストアに任せるのではなく、アプリケーションが決定することができます。結局のところ、ベクトルス\\nトアから数学的に導き出された結果に対して、クエリの本来の目的との関連性が保たれている地点を探す\\nことが重要です。\\nクエリとの関連性の高い結果ベクトルのセットを取得したら、次のプロンプトエンジニアリングのステップで\\n使用するために、それらのベクトルとともにメタデータとして保存されている関連テキストを取得します。\\n3.プロンプトエンジニアリング\\nプロンプトエンジニアリングとは、 LLMに与える命令を設計することです。一部の LLMは、プロンプトを 1つ\\nだけ受け取ります。あるいは、チャットモデルの場合に典型的ですが、命令は、プロンプトとして定義する必\\n要がある複数の異なる入力項目で構成されている場合があります。このようなプロンプトの各セクションに\\nついて、以下で個別に説明します。\\n14', metadata={'page': '13.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='カテゴリー情報を利用することができます。\\n4.エンベディングデータの生成と保存\\n先に、エンベディングに用いるためにモデルを選択しました。このモデルを用いてベクトルを生成することに\\nなります。\\nこのプロセスの最初のステップは、テキストのチャンク化です。これは、テキストをより短い文字列に分割す\\nる方法を決定するプロセスです。これは主に次の２つの理由から行われます。\\nまず、エンベディングモデルには入力として受け入れることのできるテキストの量に制限があるためです。\\ntext-ada-002 の場合、その制限は 8,192トークンです。受け入れられる文字列の長さがより短いモデルも\\nあります。\\nまた、チャンク化要否の判断は、モデルが持つ制限以外にも、データソースとアプリケーションの両方の面\\nからも検討する必要があります。アプリケーションがドキュメント全体をユーザーに返す必要がある場合、モ\\nデルが許す限り、できるだけ大きなチャンクを用いることは合理的です。あるいは、アプリケーションが、例\\nえば質疑応答システムの拡張といった場合のように、様々な事実に依存している場合、ドキュメント全体よ\\nりも、その中の数センテンス程度の短いチャンクの方が、一つのトピックに関する事実やアイデアを扱って\\nいる可能性が高く、このユースケースにより適しています。\\nさて、チャンクの長さが決まったら、次はチャンク化に関する戦略を立てる必要があります。これには多くの\\n方法があります。\\n最も簡単な方法は、テキストを均等な長さのチャンクに分割することです。この場合、重要な事実を説明し\\nている途中でチャンクが単語や文を分割してしまうことが生じる可能性があるため、チャンクのオーバーラッ\\nプを許容すると文書内の概念を適切に捉えるのに役立ちます。\\nより高度な、広く用いられている戦略として、ドキュメントの、または言語の構造に基づいたチャンク化があ\\nります。例としては、ドキュメントのセクション、段落ごと、またはセンテンスの境界を基準にしたチャンク化が\\n挙げられます。\\nドキュメントのチャンクのセットを作成したら、各チャンクのベクトルを、先の手順で定義した関連メタデータと\\n共に、データストアに保存します。これにより、近似最近傍探索（ ANN: Approximate Nearest Neighbor\\n）を利用した高速ベクトル検索が可能になります。 DataStaxの Apache Cassandraマネージドサービスで\\nあるAstraDB は、ベクトル検索の機能を有しており、大規模なベクトルデータセットを操作する場合の優れ\\nた選択肢となります。そしてまた、 AstraDBのベクトル検索機能は、高い可用性と柔軟なスケールを有した\\n分散データベース Apache Cassandraの拡張機能であるため、ベクトルとともに、そのベクトルに関連する\\nメタデータを保存しておくことも容易であり、ベクトル検索のみではなく、メタデータを使った操作との組み合\\nわせについても、利点を持っています。このような大規模データ利用のための信頼性・スケールと、データ\\nベースとしての汎用性は、 AstraDBの持つ、単なるベクトルデータベースとは異なる利点です。\\n12', metadata={'page': '11.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='ここで、より意義深い結果をユーザーに提示するために、利用可能な他のデータに基づいて検索結果を再\\nランク付けすることも考えられます。例えば、ドキュメントへのトラフィック、クリックスルー率、その他の指標\\nを用いることが考えられます。\\n●ドキュメントの要約と情報抽出\\nドキュメントの長大なリストを提示し、ユーザーにそれらを一つ一つ読ませるのではなく、ベクトル検索で見\\nつかったドキュメントを LLMを用いて要約したり、特定の情報を抽出した上で、ユーザーに提示することがで\\nきます。\\n●レコメンデーション\\nレコメンデーションシステムは、パーソナライゼーションのための強力なツールです。ユーザー毎に、その\\nユーザーが興味を持ちそうなコンテンツを確実に提示することで、システムによる顧客エンゲージメントの向\\n上を実現することができます。生成 AI本来のパワーと、プロプライエタリデータを用いて強化された汎用性と\\nの組み合わせにより、レコメンデーションシステムはさらに強力かつ広範囲をカバーするものになります。\\nレコメンデーションの実装には様々な方法がありますが、ベクトル検索を使用するアプローチのごく基本的\\nな概略は次のとおりです。\\n○アイテム・エンベディングを生成します。ここで、アイテムとはユーザーに対する推薦の候補となる\\n記事、ビデオ、写真、製品などです。アイテム・エンベディングは、コンテンツまたは製品のプロパ\\nティを内包します。これには、コンテンツであれば、ジャンルやトピックなどの属性、物理的な製品\\nであれば、色、サイズ、スタイルなどの属性が含まれます。あるいは、コンフィギュレーションパラ\\nメータ、コンビネーションルール、共通コンビネーションが、プロダクト、サービス、オファリングのた\\nめに利用される場合もあります\\n○ユーザー・エンベディングを生成します。ユーザー・エンベディングは、それぞれのユーザーの好み\\nを表現しているエンベディングベクトルです\\n○ベクトル検索を使用して、ユーザー・エンベディングに最も似通ったアイテム・エンベディングを検\\n索することで、レコメンデーションを生成できます\\nチャットやカスタマーサポート\\n生成 AIによって、チャットやカスタマーサポートアプリを拡張することができます。これによって、ユーザーの質問に対\\nしてあたかも人間が行っているかのような応答を行うことが可能になります。これによく似たアプリの例として、バー\\nチャル家庭教師やパーソナルアシスタントなどがあります。\\n重要なのは、同様のアプローチを使用して、幅広い付加機能を提供できるということです。例えば、クエリ、またはダ\\nイアログでの会話のやりとりの中で明らかになった、ユーザーの嗜好性に基づく推薦を行うことが考えられます。た\\nとえば、高齢の肉親のために最適な老人ホームを探そうとしているユーザーは、様々な要件や制約を持つことでしょ\\nう。アプリを適切なエンベディングデータで拡張することによって、 RAGベースのエクスペリエンスによって、可能な候\\n補の中からさらなる絞り込みを行った上で、ユーザーにリストを提供することができれば、ユーザーは選りすぐられた\\nリストから検討を始めることができます。\\n18', metadata={'page': '17.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='使用する LLMの選択\\nテキスト生成に使用できる LLMは多数あり、その中から利用するものを選択する必要があります。最小限のセット\\nアップですぐに使い始めたい場合は、 OpenAIモデルが良い出発点となるでしょう。 OpenAIには、様々な利用可能\\nなライブラリが用意されており24、Python、 Node.js、そして、 Azure OpenAIライブラリも利用可能です。また他の多\\nくの言語用のコミュニティ版のライブラリも存在します。\\nOpenAIモデルの中では、 gpt-4が最も強力で最高の結果が得られますが、すべてのアカウントで利用できるわけ\\nではありません。 gpt-3.5-turbo モデルも非常に強力で、はるかに安価です。コストをコントロールしながら構築を\\n進めたい場合、このモデルは多くのプロジェクトの良い出発点になるでしょう。\\n上記のどちらのモデルにも、サイズの異なるコンテキストウィンドウを備えた 2つの異なるバリエーションがありま\\nす。より大きいコンテキストウィンドウは、より多くのデータをプロンプトの入力に提供でき、より長い文書入力に耐\\nえ、ベクトル検索ステップで得られた文書サンプルをより多く利用することができます。一方、モデルのコンテキスト\\nウィンドウが大きいほど、１トークンあたりのコストも高く設定されています。そして、入力が大きくなると、請求される\\nトークンの数も増えるため、大きなコンテキストウィンドウを実際に活用する場合、コストは大幅に上昇します。その\\nため、アプリケーションに、そのような追加入力スペースが必要かどうかを判断することが重要です。\\nGoogle Cloud ユーザーの場合は、 Vertex AI25で利用できる PaLM2モデル5があり、テキストとチャットで利用でき\\nます。オープンソースの選択肢については、 Hugging Faceによる Open LLM Leaderboard7をご覧ください。\\nユースケース\\nセマンティック検索\\nセマンティック検索は、しばしば「意味や理解を伴う検索」と説明されます。それは、単語や語句を正確に一致させよ\\nうとする語彙検索やキーワード検索とは対照的です。\\nセマンティック検索の威力を伝える、いくつかの簡単な例を示します。「ウェブサイトのテーマ」、「ウェブサイトのテン\\nプレート」、および「ウェブサイトのデザイン」というフレーズはすべて、ウェブサイトの視覚的なプレゼンテーション（見\\nた目）を表現しています。ただし、従来の語彙検索で「テーマ」というキーワードを使って検索すると、特定の製品が\\nそれらを「テンプレート」と呼んでいる場合、そうした情報は検索結果として返されません。あるいは、画像の検索に\\nセマンティック検索を用いる場合、「ハヤブサ」と「タカ」は、どちらも鳥であるため、ベクトル空間の近接した位置に配\\n置され、お互いに検索が可能である一方、これらは「家」の画像とはかけ離れたものとして扱われます。\\nセマンティック検索で解決できる問題の種類は数多くあります。いくつかの例を次に示します。\\n●検索エンジン\\nおそらく疑問を挟む余地のないところとして、セマンティック検索の技術を用いて構築された検索エンジンは\\n非常に強力かつ高速なものになります。\\nこの場合、単純にベクトル検索を実行して、検索結果の上位 N件のデータを返します。ここで、 Nは返した\\nいドキュメントの数です。あるベクトルに類似したベクトルの検索には、通常、近似最近傍（ ANN）検索が用\\nいられます。\\n17', metadata={'page': '16.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='より複雑なアプリケーションでは、タスクを複数のサブタスクに分割した上で、問題を解決するために外部ツールを使\\n用する必要があるかどうかを判断したり、アプリケーション用のメモリを使ってデータを保存し維持するために、エー\\nジェントが利用される場合があります。\\n図3.1RAGパターンを用いたアーキテクチャのコンテキストビュー\\nここで、エンベディングデータの保存とベクトル検索の実行にベクトル対応データベースを採用すると、利用可能な\\nデータ量を非常に大規模なデータセットに拡張しながら、ドキュメント検索速度の性能を維持できるという利点を得る\\nことができます。ベクトル対応データベースは、あらゆる生成エクスペリエンスの基本要件である、類似性検索を高\\nい効率で実現する機能を備えています。\\nベクトル対応データベースの採用\\nSequoia Capital 社の最近の調査10によると、生成 AIに取り組む 88%の企業が、生成 AIアプリ\\nケーションの実現に向けて、ドキュメント検索メカニズムが、今後インフラストラクチャを構成する\\n重要な部分になると考えています。\\n9', metadata={'page': '8.0', 'source': './DataStax_Vector-Search.pdf'})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['result'])"
      ],
      "metadata": {
        "id": "MmI2usFZ9QM3",
        "outputId": "1aec300a-3f96-4d39-ca25-34791a3888c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ベクトル検索は、ベクトル演算を使用してクエリに最も類似するデータを検索する方法です。クエリの表現をベクトル化し、そのベクトルを使用してデータのベクトルとの類似性を計算します。類似性に基づいてデータを検索することで、効率的な検索が実現されます。ベクトル検索は、セマンティック情報を保持するベクトル演算の一例であり、意味概念を捉えることが期待されます。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['source_documents'])"
      ],
      "metadata": {
        "id": "sDWE3gf_54aZ",
        "outputId": "1c227880-1410-4452-de67-c60eaa0b9643",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='け、ということが見られます。このような場合に、ベクトル検索を活用することにより、クエリを意味レベルで把握し、そ\\nの内容に応じてキャッシュから結果を検索することができます。\\nLLMキャッシュのシンプルな実装には、次の手順が必要です。\\n1. LLM に送信されたクエリごとに、エンベディングを生成します\\n2.クエリに対する LLMの応答を保存します\\n3.両方のエンベディングをベクターデータストアに保存します\\n4. LLM を呼び出す前に、ベクトル検索を使用して同様のクエリが以前にすでに実行されキャッシュされていな\\nいかを確認し、存在する場合はキャッシュされた結果を返します\\nこの手法により、本番アプリケーションで LLMを実行する際のコストとレイテンシの問題を大幅に削減できます。\\nただし、欠点の 1つとして、キャッシュされた応答に個人情報が保存され、別のユーザーの応答に利用されてしま\\nう、ということが起きないように細心の注意を払う必要があります。\\nエンベディングによる異常検出\\n異常（アノマリー）検出とは、特定のデータセット内で予想される動作、または通常の動作から大きく逸脱するパター\\nン、あるいはポイントを特定するプロセスを指します。例えば、典型的なパターンに従わない外れ値の検出や、通常\\nと異なる、あるいは非常に稀なイベントの検出、などがあります。\\n密度ベースの異常検出にベクトル検索を使用する簡単なプロセスは次のとおりです。\\n●アプリケーションが管理しているデータのエンベディングデータセットを作成\\n●アプリケーションへの入力（ドキュメントまたは画像）に対して、そのエンベディングデータを生成\\n●アプリケーションの既存のデータセットの中から入力されたエンベディングデータに最も近いものを検索\\n●入力データと検索されたデータとの間の距離が、ノーマルな範囲からの逸脱を示す、しきい値を超えている\\nかどうかを確認\\n●入力データを異常（アノマリー）としてラベル付け\\n22', metadata={'page': '21.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='この例では、ベクトルは 2次元の情報を持ち、ベクトルデータを構成するそれぞれの要素は、次の２つの情報を表し\\nてます： [<衣服の種類 >, <服の色>]。ここで、「 赤いシャツ」のベクトルは [1, 1]で、「青いズボン」のベクトルは\\n[-1, -1]で表されているとします。そこに、「緑色のスカート」を加えます。スカートは機能的にはズボンに近く、緑は色\\nのスペクトルとして青に近いため、そのエンベディングベクトルは「青いズボン」に近い位置にあるはずで、例えば\\n[-0.8, -0.8] のような値を取ると推測することができます。\\n上の例は、エンベディングを使ってベクトル空間を構築する方法と、その空間の中で、オブジェクトが存在する場所\\nについてのアイディアを示しています。\\n次に、エンベディングに対する算術演算がどのように機能するかを示す例を見てみましょう。予め注意しておくと、ベ\\nクトル空間は実際には非常に大きな次元を持つ（ベクトルは多くの数値で構成される）のが一般的です。ここで扱う\\nベクトル空間は、「人物の肩書き」と「性別」という二つの概念を捉えた情報を扱っています。図 2を参照ください。\\n6', metadata={'page': '5.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='図2.セマンティック情報を保持するベクトル演算の例\\n埋め込み（エンベディング）表現はベクトルであるため、ベクトル同士で算術演算を行うことができます。その際、その\\n演算結果のベクトルには、ベクトルの元となった自然言語の組み合わせに相当する意味概念が保持されることが期\\n待されます。\\nたとえば、 ベクトル同士の計算において、 《「王子」-「男性」+「女性」》 という計算式の結果として得\\nられるベクトルは、 「プリンセス」を表すベクトルと類似する値であると考えられます 。\\n同様に、《 「王子」-「男性」》 と《「プリンセス」 -「女性」》という２つの計算式の結果のベクトル\\nは、互いに非常に近い距離にあるでしょう。この場合、これらのベクトルは、「王または女王の直系の子孫である王\\n族（男女の別を問わない）」という概念を捉えていると見ることができます。\\n検索拡張生成（ RAG）パターン\\nLLMには、それが素のままで用いられた場合、生成された応答が特定のアプリケーションでは用をなさない可能性\\nがある、という限界があります。これは、 LLMに保存されている知識が通常そのモデルのトレーニングに用いられた\\nデータに限定されており、インターネット上で一般に入手できないような、最新のデータや、特定のドメインに関連す\\nるプロプライエタリデータが含まれていないことに由来します。\\n検索拡張生成（ RAG）の中心となるアイディアは、 LLMと、ユーザーが実行を依頼するタスクの解決に関連する独自\\nのドキュメントとを統合することです。\\n図3は、 RAGを用いたアプリケーションがどのように動作するかを示しています。まず、クエリがアプリケーションに対\\nしてサブミットされます。生成 AIアプリでは、クエリの表現は、検索エンジンタイプ、つまり「ジョージ・ワシントンの歴\\n史」というようなキーワード、または「ジョージ・ワシントンはいつ生まれたか」のような具体的な質問文、さらには\\n「ジョージ・ワシントンの生涯についてのエッセイを書け」という命令文、といったように様々な形式をとり得ます。独自\\nデータソースのエンベディングに使用したものと同じエンベディングモデルを使用してクエリのエンベディングを行い、\\nそのクエリのエンベディングを使用してベクトル検索を行うことによって、そのクエリに最も類似するデータを検索す\\nることができます。このようなデータ間の類似性に基づく効率的な検索は、ベクトル検索によって実現される重要な\\n機能です。\\n7', metadata={'page': '6.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='ホワイトペーパー\\n生成 AIアプリ\\nのための\\nベクトル検索\\nAIアプリケーション開発にベクトル検索を活用す\\nるための開発者/アーキテクト向けガイド\\nこのドキュメントは、生成 AIアプリケーションを企業独\\n自のデータと組み合わせて、設計・構築しようとしてい\\nる全ての方のためのガイドです。組織が理解すべき重\\n要な概念と考慮事項を取り扱うだけでなく、ベクトル検\\n索を用いて、 LLMの持つ機能を大幅に拡張するため\\nのシンプルで強力なアプローチについても解説しま\\nす。', metadata={'page': '0.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='しかし、 LLMに対して、いくら情報を与えたところで、それらが課題に関連する情報でない場合、役には立ちません。\\n意味的に課題に関連する情報（ semantically relevant information）を与えることが重要になります。ベクトル検索\\nを使えば、そのような情報を特定することは驚くほど簡単になります。\\nベクトル検索とは何か ?\\nベクトル検索は、類似した属性または特徴を持つ関連オブジェクトを検索する方法です。ベクトル検索を活用するこ\\nとができるオブジェクトの一般的な例には、テキスト、物理的な製品、画像、ビデオなどがあります。\\nベクトル検索では、エンベディングと呼ばれる特定のタイプの機械学習モデルを使用して、オブジェクトとそのコンテ\\nキストを記述します。エンベディングされたデータは、オブジェクトのセマンティクスを捉えるベクトルであり、次のセク\\nションで詳しく説明します。このアプローチにより、ベクトル検索では、正確にどの部分が類似する対象を探すことに\\nなるかについて前知識を持つ必要がなく、類似したデータを見つけることができます。\\n英語を例に挙げると、「 happy」、「 cheerful」、「 joyful」という単語はすべて同様の意味を持ちますが、従来のキー\\nワードベースの検索では、「 happy」というキーワードを用いた場合、「 cheerful」と「 joyful」に一致するドキュメントは\\n検索されません。これを解決するのが、ベクトル検索の能力です。ベクトル検索は意味を理解することができるた\\nめ、ユーザーが網羅的に指示せずとも、検索したい対象を伝えることができます。\\nもう一つの利点として、ベクトル検索は、近似最近傍 (ANN)検索と呼ばれるアルゴリズムのセットを使用して非常に\\n高速に実行できることがあります。このアプローチは非常に効率的であり、数学的に互いに類似するベクトルを素早\\nく導出します。\\nエンベディング（埋め込み）の基礎\\nエンベディング（埋め込み）は、テキストの意味を封じ込めたデータの数学的表現です。テキストをベクトル化して埋\\nめ込み（エンベディング）表現に変換するために、ドキュメントを構成する単語群から数値のリストへの変換を実施し\\nます。エンベディングの結果、ベクトル空間内では、互いに関連性の高い意味を持つベクトル同士は、近接した場所\\nに配置されることになります。\\nもっと分かりやすくするため、例を用いて考えてみましょう。図 1は、「衣服の種類」に関する次元と「服の色」という\\n属性に関する次元の 2つの次元を持つ単純なベクトル空間を示しています。\\n図1.衣服を表現する 2次元ベクトル空間\\n5', metadata={'page': '4.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='1.クエリのエンベディング\\nRAGアプリケーションにおけるテキスト生成はクエリから始まります。クエリはアプリケーションのエンド\\nユーザーから送信されたものを、そのまま利用する場合もあれば、それを元に LLMまたはその他の手続き\\n型コードによって新たに生成する場合もあります。\\n次に、クエリのテキストエンベディングを行って、クエリをベクトルにエンコードします。\\nここで使用されるエンベディングモデルが、ベクトル検索用のデータセットをエンベディングするために使用\\nされたものと同じモデルであることが重要です。そうでない場合、ベクトル検索は正しく機能しません。\\n2.ベクトル検索\\n次に、ベクトルデータストアからクエリに最も類似したエンベディングデータを取得することになります。ま\\nず、クエリベクトルの近くにあるベクトルを探すため、最近傍検索を行います。多くの場合、この検索は「上\\n位N件」の一致を求めることによって行われます。例えば、上位 3件の一致を利用する、というようなことが\\n一般に行われますが、この数は、エンベディングで使用したチャンクの長さ、コンテキストウィンドウ（ LLMに\\n許可される入力のサイズ）、およびユースケース等に応じて調整することになります。\\nベクトル検索を行う際に、検索結果のみでなく、クエリベクトルと検索結果ベクトルとの間の類似性について\\nのスコアまたは尺度を返すように要求することがあります。このスコアがしきい値を下回る場合、検索され\\nたドキュメントのクエリに対する一致度が十分なものでないとして、後続のプロセスで、この検索結果ドキュ\\nメントを、次のステップでは利用しないという決定を行うことができます。使用する実際のしきい値は、アプリ\\nケーション、データセット、および類似性メトリックによって異なります。スコアの範囲が、最大値を「 1」として\\n正規化されていると仮定する場合、カットオフの値として、「 0.5～0.8」の間から実験を開始するのが適切\\nと言えるでしょう。\\nベクトル検索により、たとえば数百、数千件規模の一致結果を取得すると共に、それぞれのスコアを得た\\n上で、エンベディングの生成に使用された元テキストを使って選別を行うことで、適切なカットオフを、ベクト\\nルデータストアに任せるのではなく、アプリケーションが決定することができます。結局のところ、ベクトルス\\nトアから数学的に導き出された結果に対して、クエリの本来の目的との関連性が保たれている地点を探す\\nことが重要です。\\nクエリとの関連性の高い結果ベクトルのセットを取得したら、次のプロンプトエンジニアリングのステップで\\n使用するために、それらのベクトルとともにメタデータとして保存されている関連テキストを取得します。\\n3.プロンプトエンジニアリング\\nプロンプトエンジニアリングとは、 LLMに与える命令を設計することです。一部の LLMは、プロンプトを 1つ\\nだけ受け取ります。あるいは、チャットモデルの場合に典型的ですが、命令は、プロンプトとして定義する必\\n要がある複数の異なる入力項目で構成されている場合があります。このようなプロンプトの各セクションに\\nついて、以下で個別に説明します。\\n14', metadata={'page': '13.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='カテゴリー情報を利用することができます。\\n4.エンベディングデータの生成と保存\\n先に、エンベディングに用いるためにモデルを選択しました。このモデルを用いてベクトルを生成することに\\nなります。\\nこのプロセスの最初のステップは、テキストのチャンク化です。これは、テキストをより短い文字列に分割す\\nる方法を決定するプロセスです。これは主に次の２つの理由から行われます。\\nまず、エンベディングモデルには入力として受け入れることのできるテキストの量に制限があるためです。\\ntext-ada-002 の場合、その制限は 8,192トークンです。受け入れられる文字列の長さがより短いモデルも\\nあります。\\nまた、チャンク化要否の判断は、モデルが持つ制限以外にも、データソースとアプリケーションの両方の面\\nからも検討する必要があります。アプリケーションがドキュメント全体をユーザーに返す必要がある場合、モ\\nデルが許す限り、できるだけ大きなチャンクを用いることは合理的です。あるいは、アプリケーションが、例\\nえば質疑応答システムの拡張といった場合のように、様々な事実に依存している場合、ドキュメント全体よ\\nりも、その中の数センテンス程度の短いチャンクの方が、一つのトピックに関する事実やアイデアを扱って\\nいる可能性が高く、このユースケースにより適しています。\\nさて、チャンクの長さが決まったら、次はチャンク化に関する戦略を立てる必要があります。これには多くの\\n方法があります。\\n最も簡単な方法は、テキストを均等な長さのチャンクに分割することです。この場合、重要な事実を説明し\\nている途中でチャンクが単語や文を分割してしまうことが生じる可能性があるため、チャンクのオーバーラッ\\nプを許容すると文書内の概念を適切に捉えるのに役立ちます。\\nより高度な、広く用いられている戦略として、ドキュメントの、または言語の構造に基づいたチャンク化があ\\nります。例としては、ドキュメントのセクション、段落ごと、またはセンテンスの境界を基準にしたチャンク化が\\n挙げられます。\\nドキュメントのチャンクのセットを作成したら、各チャンクのベクトルを、先の手順で定義した関連メタデータと\\n共に、データストアに保存します。これにより、近似最近傍探索（ ANN: Approximate Nearest Neighbor\\n）を利用した高速ベクトル検索が可能になります。 DataStaxの Apache Cassandraマネージドサービスで\\nあるAstraDB は、ベクトル検索の機能を有しており、大規模なベクトルデータセットを操作する場合の優れ\\nた選択肢となります。そしてまた、 AstraDBのベクトル検索機能は、高い可用性と柔軟なスケールを有した\\n分散データベース Apache Cassandraの拡張機能であるため、ベクトルとともに、そのベクトルに関連する\\nメタデータを保存しておくことも容易であり、ベクトル検索のみではなく、メタデータを使った操作との組み合\\nわせについても、利点を持っています。このような大規模データ利用のための信頼性・スケールと、データ\\nベースとしての汎用性は、 AstraDBの持つ、単なるベクトルデータベースとは異なる利点です。\\n12', metadata={'page': '11.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='ここで、より意義深い結果をユーザーに提示するために、利用可能な他のデータに基づいて検索結果を再\\nランク付けすることも考えられます。例えば、ドキュメントへのトラフィック、クリックスルー率、その他の指標\\nを用いることが考えられます。\\n●ドキュメントの要約と情報抽出\\nドキュメントの長大なリストを提示し、ユーザーにそれらを一つ一つ読ませるのではなく、ベクトル検索で見\\nつかったドキュメントを LLMを用いて要約したり、特定の情報を抽出した上で、ユーザーに提示することがで\\nきます。\\n●レコメンデーション\\nレコメンデーションシステムは、パーソナライゼーションのための強力なツールです。ユーザー毎に、その\\nユーザーが興味を持ちそうなコンテンツを確実に提示することで、システムによる顧客エンゲージメントの向\\n上を実現することができます。生成 AI本来のパワーと、プロプライエタリデータを用いて強化された汎用性と\\nの組み合わせにより、レコメンデーションシステムはさらに強力かつ広範囲をカバーするものになります。\\nレコメンデーションの実装には様々な方法がありますが、ベクトル検索を使用するアプローチのごく基本的\\nな概略は次のとおりです。\\n○アイテム・エンベディングを生成します。ここで、アイテムとはユーザーに対する推薦の候補となる\\n記事、ビデオ、写真、製品などです。アイテム・エンベディングは、コンテンツまたは製品のプロパ\\nティを内包します。これには、コンテンツであれば、ジャンルやトピックなどの属性、物理的な製品\\nであれば、色、サイズ、スタイルなどの属性が含まれます。あるいは、コンフィギュレーションパラ\\nメータ、コンビネーションルール、共通コンビネーションが、プロダクト、サービス、オファリングのた\\nめに利用される場合もあります\\n○ユーザー・エンベディングを生成します。ユーザー・エンベディングは、それぞれのユーザーの好み\\nを表現しているエンベディングベクトルです\\n○ベクトル検索を使用して、ユーザー・エンベディングに最も似通ったアイテム・エンベディングを検\\n索することで、レコメンデーションを生成できます\\nチャットやカスタマーサポート\\n生成 AIによって、チャットやカスタマーサポートアプリを拡張することができます。これによって、ユーザーの質問に対\\nしてあたかも人間が行っているかのような応答を行うことが可能になります。これによく似たアプリの例として、バー\\nチャル家庭教師やパーソナルアシスタントなどがあります。\\n重要なのは、同様のアプローチを使用して、幅広い付加機能を提供できるということです。例えば、クエリ、またはダ\\nイアログでの会話のやりとりの中で明らかになった、ユーザーの嗜好性に基づく推薦を行うことが考えられます。た\\nとえば、高齢の肉親のために最適な老人ホームを探そうとしているユーザーは、様々な要件や制約を持つことでしょ\\nう。アプリを適切なエンベディングデータで拡張することによって、 RAGベースのエクスペリエンスによって、可能な候\\n補の中からさらなる絞り込みを行った上で、ユーザーにリストを提供することができれば、ユーザーは選りすぐられた\\nリストから検討を始めることができます。\\n18', metadata={'page': '17.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='使用する LLMの選択\\nテキスト生成に使用できる LLMは多数あり、その中から利用するものを選択する必要があります。最小限のセット\\nアップですぐに使い始めたい場合は、 OpenAIモデルが良い出発点となるでしょう。 OpenAIには、様々な利用可能\\nなライブラリが用意されており24、Python、 Node.js、そして、 Azure OpenAIライブラリも利用可能です。また他の多\\nくの言語用のコミュニティ版のライブラリも存在します。\\nOpenAIモデルの中では、 gpt-4が最も強力で最高の結果が得られますが、すべてのアカウントで利用できるわけ\\nではありません。 gpt-3.5-turbo モデルも非常に強力で、はるかに安価です。コストをコントロールしながら構築を\\n進めたい場合、このモデルは多くのプロジェクトの良い出発点になるでしょう。\\n上記のどちらのモデルにも、サイズの異なるコンテキストウィンドウを備えた 2つの異なるバリエーションがありま\\nす。より大きいコンテキストウィンドウは、より多くのデータをプロンプトの入力に提供でき、より長い文書入力に耐\\nえ、ベクトル検索ステップで得られた文書サンプルをより多く利用することができます。一方、モデルのコンテキスト\\nウィンドウが大きいほど、１トークンあたりのコストも高く設定されています。そして、入力が大きくなると、請求される\\nトークンの数も増えるため、大きなコンテキストウィンドウを実際に活用する場合、コストは大幅に上昇します。その\\nため、アプリケーションに、そのような追加入力スペースが必要かどうかを判断することが重要です。\\nGoogle Cloud ユーザーの場合は、 Vertex AI25で利用できる PaLM2モデル5があり、テキストとチャットで利用でき\\nます。オープンソースの選択肢については、 Hugging Faceによる Open LLM Leaderboard7をご覧ください。\\nユースケース\\nセマンティック検索\\nセマンティック検索は、しばしば「意味や理解を伴う検索」と説明されます。それは、単語や語句を正確に一致させよ\\nうとする語彙検索やキーワード検索とは対照的です。\\nセマンティック検索の威力を伝える、いくつかの簡単な例を示します。「ウェブサイトのテーマ」、「ウェブサイトのテン\\nプレート」、および「ウェブサイトのデザイン」というフレーズはすべて、ウェブサイトの視覚的なプレゼンテーション（見\\nた目）を表現しています。ただし、従来の語彙検索で「テーマ」というキーワードを使って検索すると、特定の製品が\\nそれらを「テンプレート」と呼んでいる場合、そうした情報は検索結果として返されません。あるいは、画像の検索に\\nセマンティック検索を用いる場合、「ハヤブサ」と「タカ」は、どちらも鳥であるため、ベクトル空間の近接した位置に配\\n置され、お互いに検索が可能である一方、これらは「家」の画像とはかけ離れたものとして扱われます。\\nセマンティック検索で解決できる問題の種類は数多くあります。いくつかの例を次に示します。\\n●検索エンジン\\nおそらく疑問を挟む余地のないところとして、セマンティック検索の技術を用いて構築された検索エンジンは\\n非常に強力かつ高速なものになります。\\nこの場合、単純にベクトル検索を実行して、検索結果の上位 N件のデータを返します。ここで、 Nは返した\\nいドキュメントの数です。あるベクトルに類似したベクトルの検索には、通常、近似最近傍（ ANN）検索が用\\nいられます。\\n17', metadata={'page': '16.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='より複雑なアプリケーションでは、タスクを複数のサブタスクに分割した上で、問題を解決するために外部ツールを使\\n用する必要があるかどうかを判断したり、アプリケーション用のメモリを使ってデータを保存し維持するために、エー\\nジェントが利用される場合があります。\\n図3.1RAGパターンを用いたアーキテクチャのコンテキストビュー\\nここで、エンベディングデータの保存とベクトル検索の実行にベクトル対応データベースを採用すると、利用可能な\\nデータ量を非常に大規模なデータセットに拡張しながら、ドキュメント検索速度の性能を維持できるという利点を得る\\nことができます。ベクトル対応データベースは、あらゆる生成エクスペリエンスの基本要件である、類似性検索を高\\nい効率で実現する機能を備えています。\\nベクトル対応データベースの採用\\nSequoia Capital 社の最近の調査10によると、生成 AIに取り組む 88%の企業が、生成 AIアプリ\\nケーションの実現に向けて、ドキュメント検索メカニズムが、今後インフラストラクチャを構成する\\n重要な部分になると考えています。\\n9', metadata={'page': '8.0', 'source': './DataStax_Vector-Search.pdf'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"RAGにおけるエンベディングについて説明して\"\n",
        "response = qa(query)\n",
        "print(\"回答：\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "K4ajJoxC95V2",
        "outputId": "f56ab7c9-d51e-44c7-b283-56e5681a6316",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:349: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "回答：\n",
            "{'query': 'RAGにおけるエンベディングについて説明して', 'result': 'RAGアプリケーションでは、テキスト生成はクエリから始まります。クエリはエンドユーザーから送信されたものを利用する場合もありますが、LLMや他の手続き型コードを使用して新たに生成することもあります。次に、クエリのテキストエンベディングを行い、クエリをベクトルにエンコードします。重要なのは、ベクトル検索用のデータセットをエンベディングするために使用されるエンベディングモデルが、同じモデルであることです。その後、ベクトル検索を行い、クエリに最も類似したエンベディングデータを取得します。検索結果には、類似性についてのスコアや尺度も含まれることがあります。最後に、プロンプトエンジニアリングを行い、LLMに与える命令を設計します。', 'source_documents': [Document(page_content='RAG AIアプリケーションの実装\\n検索拡張生成（ RAG）を用いた生成 AIアプリを構築するための、基本的なフロー（図 3）とアーキテクチャダイアグラ\\nム（図 3.1）を紹介しました。ここでは、それらをさらに詳しく見てみましょう。\\nRAGシステムを構築するには、次の 2つのワークフローが必要です。\\n●ナレッジベースの作成\\n●生成プロセスの実行\\n以下、それぞれについて見ていきます。\\nナレッジベースの作成\\nまず、 RAGアプリケーションがアクセスする、エンベディングデータからなるナレッジベースの構築について見てみま\\nしょう。このワークフローには複数のステップがあり、その概要を図 4に示します。\\n図4.データセットをベクトルストアに変換するワークフロー\\n1.プロプライエタリデータの特定\\n初めのステップは、どのようなアプリケーションを構築しているかによって異なってきます。まず、アプリケー\\nションで使用できそうなデータセットを検討することになりますが、その際には、そのデータソースはどのよう\\nな固有の情報を提供するか？、エンベディングに利用できる公開データセットはあるか？、といった点を検\\n討する必要があります。\\n公開データセットの中には、 LLM構築時のトレーニングに用いられたデータセットよりも新しいデータが含ま\\nれている可能性があり、新たに、そうしたデータセットを用いることも考えられます。たとえば、最近のニュー\\nス記事や最新の開発者用ドキュメントといったものがあります。\\n2.エンベディングモデルの選択\\nエンベディングモデルの選択において、ナレッジベース用のエンベディングモデルと最終的な応答のテキス\\n10', metadata={'page': '9.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='ここで、より意義深い結果をユーザーに提示するために、利用可能な他のデータに基づいて検索結果を再\\nランク付けすることも考えられます。例えば、ドキュメントへのトラフィック、クリックスルー率、その他の指標\\nを用いることが考えられます。\\n●ドキュメントの要約と情報抽出\\nドキュメントの長大なリストを提示し、ユーザーにそれらを一つ一つ読ませるのではなく、ベクトル検索で見\\nつかったドキュメントを LLMを用いて要約したり、特定の情報を抽出した上で、ユーザーに提示することがで\\nきます。\\n●レコメンデーション\\nレコメンデーションシステムは、パーソナライゼーションのための強力なツールです。ユーザー毎に、その\\nユーザーが興味を持ちそうなコンテンツを確実に提示することで、システムによる顧客エンゲージメントの向\\n上を実現することができます。生成 AI本来のパワーと、プロプライエタリデータを用いて強化された汎用性と\\nの組み合わせにより、レコメンデーションシステムはさらに強力かつ広範囲をカバーするものになります。\\nレコメンデーションの実装には様々な方法がありますが、ベクトル検索を使用するアプローチのごく基本的\\nな概略は次のとおりです。\\n○アイテム・エンベディングを生成します。ここで、アイテムとはユーザーに対する推薦の候補となる\\n記事、ビデオ、写真、製品などです。アイテム・エンベディングは、コンテンツまたは製品のプロパ\\nティを内包します。これには、コンテンツであれば、ジャンルやトピックなどの属性、物理的な製品\\nであれば、色、サイズ、スタイルなどの属性が含まれます。あるいは、コンフィギュレーションパラ\\nメータ、コンビネーションルール、共通コンビネーションが、プロダクト、サービス、オファリングのた\\nめに利用される場合もあります\\n○ユーザー・エンベディングを生成します。ユーザー・エンベディングは、それぞれのユーザーの好み\\nを表現しているエンベディングベクトルです\\n○ベクトル検索を使用して、ユーザー・エンベディングに最も似通ったアイテム・エンベディングを検\\n索することで、レコメンデーションを生成できます\\nチャットやカスタマーサポート\\n生成 AIによって、チャットやカスタマーサポートアプリを拡張することができます。これによって、ユーザーの質問に対\\nしてあたかも人間が行っているかのような応答を行うことが可能になります。これによく似たアプリの例として、バー\\nチャル家庭教師やパーソナルアシスタントなどがあります。\\n重要なのは、同様のアプローチを使用して、幅広い付加機能を提供できるということです。例えば、クエリ、またはダ\\nイアログでの会話のやりとりの中で明らかになった、ユーザーの嗜好性に基づく推薦を行うことが考えられます。た\\nとえば、高齢の肉親のために最適な老人ホームを探そうとしているユーザーは、様々な要件や制約を持つことでしょ\\nう。アプリを適切なエンベディングデータで拡張することによって、 RAGベースのエクスペリエンスによって、可能な候\\n補の中からさらなる絞り込みを行った上で、ユーザーにリストを提供することができれば、ユーザーは選りすぐられた\\nリストから検討を始めることができます。\\n18', metadata={'page': '17.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='1.クエリのエンベディング\\nRAGアプリケーションにおけるテキスト生成はクエリから始まります。クエリはアプリケーションのエンド\\nユーザーから送信されたものを、そのまま利用する場合もあれば、それを元に LLMまたはその他の手続き\\n型コードによって新たに生成する場合もあります。\\n次に、クエリのテキストエンベディングを行って、クエリをベクトルにエンコードします。\\nここで使用されるエンベディングモデルが、ベクトル検索用のデータセットをエンベディングするために使用\\nされたものと同じモデルであることが重要です。そうでない場合、ベクトル検索は正しく機能しません。\\n2.ベクトル検索\\n次に、ベクトルデータストアからクエリに最も類似したエンベディングデータを取得することになります。ま\\nず、クエリベクトルの近くにあるベクトルを探すため、最近傍検索を行います。多くの場合、この検索は「上\\n位N件」の一致を求めることによって行われます。例えば、上位 3件の一致を利用する、というようなことが\\n一般に行われますが、この数は、エンベディングで使用したチャンクの長さ、コンテキストウィンドウ（ LLMに\\n許可される入力のサイズ）、およびユースケース等に応じて調整することになります。\\nベクトル検索を行う際に、検索結果のみでなく、クエリベクトルと検索結果ベクトルとの間の類似性について\\nのスコアまたは尺度を返すように要求することがあります。このスコアがしきい値を下回る場合、検索され\\nたドキュメントのクエリに対する一致度が十分なものでないとして、後続のプロセスで、この検索結果ドキュ\\nメントを、次のステップでは利用しないという決定を行うことができます。使用する実際のしきい値は、アプリ\\nケーション、データセット、および類似性メトリックによって異なります。スコアの範囲が、最大値を「 1」として\\n正規化されていると仮定する場合、カットオフの値として、「 0.5～0.8」の間から実験を開始するのが適切\\nと言えるでしょう。\\nベクトル検索により、たとえば数百、数千件規模の一致結果を取得すると共に、それぞれのスコアを得た\\n上で、エンベディングの生成に使用された元テキストを使って選別を行うことで、適切なカットオフを、ベクト\\nルデータストアに任せるのではなく、アプリケーションが決定することができます。結局のところ、ベクトルス\\nトアから数学的に導き出された結果に対して、クエリの本来の目的との関連性が保たれている地点を探す\\nことが重要です。\\nクエリとの関連性の高い結果ベクトルのセットを取得したら、次のプロンプトエンジニアリングのステップで\\n使用するために、それらのベクトルとともにメタデータとして保存されている関連テキストを取得します。\\n3.プロンプトエンジニアリング\\nプロンプトエンジニアリングとは、 LLMに与える命令を設計することです。一部の LLMは、プロンプトを 1つ\\nだけ受け取ります。あるいは、チャットモデルの場合に典型的ですが、命令は、プロンプトとして定義する必\\n要がある複数の異なる入力項目で構成されている場合があります。このようなプロンプトの各セクションに\\nついて、以下で個別に説明します。\\n14', metadata={'page': '13.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='より複雑なアプリケーションでは、タスクを複数のサブタスクに分割した上で、問題を解決するために外部ツールを使\\n用する必要があるかどうかを判断したり、アプリケーション用のメモリを使ってデータを保存し維持するために、エー\\nジェントが利用される場合があります。\\n図3.1RAGパターンを用いたアーキテクチャのコンテキストビュー\\nここで、エンベディングデータの保存とベクトル検索の実行にベクトル対応データベースを採用すると、利用可能な\\nデータ量を非常に大規模なデータセットに拡張しながら、ドキュメント検索速度の性能を維持できるという利点を得る\\nことができます。ベクトル対応データベースは、あらゆる生成エクスペリエンスの基本要件である、類似性検索を高\\nい効率で実現する機能を備えています。\\nベクトル対応データベースの採用\\nSequoia Capital 社の最近の調査10によると、生成 AIに取り組む 88%の企業が、生成 AIアプリ\\nケーションの実現に向けて、ドキュメント検索メカニズムが、今後インフラストラクチャを構成する\\n重要な部分になると考えています。\\n9', metadata={'page': '8.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='最後に、アプリは検索に該当した上位数件のデータのテキストを取得し、それらを LLMに与える命令文の作成に活\\n用して、 LLMにタスクの結果を生成させます。\\n図3.RAGパターンを用いた LLMアプリケーションの基本フロー\\nこのアプローチには、次の利点があります。\\n●ハルシネーションの軽減\\nAIの分野では、一見すると尤もらしく見えるものの、事実とは異なる、あるいは文脈に適さない回答が出力\\nされることを「ハルシネーション（ Hallucination：幻覚）」と呼びます。 RAGデザインパターンは、ベクトル検\\n索によって抽出されたドキュメントに焦点を当てるように LLMに指示して、応答を生成します。 LLMのト\\nレーニングに用いられたデータセットの全体ではなく、より文脈に適したドキュメントセットを使用することで、\\nクエリへの応答は、ユーザーの意図をより正確に汲み取ったものになります。\\n●プロプライエタリデータの活用\\n企業が LLMアプリケーションを開発する場合、 LLMの生成する応答を、その企業の製品 ・サービス とユース\\nケースに対応した内容にするために、そのアプリケーションの要件に関連した企業の保有する情報・ドキュ\\nメントを、 LLMに直接提供することができます。\\n●LLM「トレーニング」の簡便さ\\nRAGパターンは、特定のアプリケーションに合わせてモデルを調整する方法の中でも、実装および保守が\\n容易です。プロプライエタリデータへの新しいドキュメントの追加、既存のドキュメントの変更、または新しい\\nデータソースが利用可能になったときにも、エンベディングデータを更新して、モデルを最新の状態に、そし\\nてその応答を正確なものに保つことが容易です。\\n次の図 3.1は、 RAGを使用したアプリケーションまたはサービスを実行するための完全なアーキテクチャの概要を示\\nしています。このアーキテクチャでは、エージェントという新たな概念が導入されています。エージェントとは、自動化\\nされた推論・意思決定エンジンです。エージェントは、ユーザーからの入力すなわちクエリを受け取り、それに応じて\\n何を行うかを決定するアプリケーション・コンポーネントまたはプログラミング・コードです。\\n図3で紹介した単純な RAGの例では、エージェントのタスクは、ユーザークエリのエンベディング、ベクトル検索の実\\n行、検索結果を含む LLMのプロンプトの作成といった一連の基本的なプロセスからなっていました。\\n8', metadata={'page': '7.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='5.エンベディングデータの更新\\n企業が保有するデータは、時間の経過とともに変化していきます。ハルシネーションを避けるためには、新\\nしいデータソースが利用可能になったらすぐに、エンベディングデータを更新することが重要です。\\nデータソースの変更が発生するのは、ある時は、新しいドキュメントが利用可能になったためかもしれませ\\nんし、またある時は、過去にエンコード済みのコンテンツが更新されたためである場合もあります。\\n既存のドキュメントのエンベディングデータを更新するときは、企業が保有するデータの１オブジェクトに対し\\nて複数のエンベディングが登録されている可能性があることに注意する必要があります。データソースがテ\\nキストである場合、これはテキスト全体の長さと選択したチャンク化の戦略によって異なります。エンベディ\\nングデータを更新するときは、更新時にそのドキュメントから派生した、すべてのエンベディングデータを置\\nき換えるように注意する必要があります。\\nまた、すべてのエンベディングモデルの変換ロジックが決定論的であるわけではないことを覚えておくこと\\nは有益です。モデルによっては、同じテキストをエンコードする都度、異なる結果が得られる場合がありま\\nす。結果として得られるベクトルは通常、類似したものにはなるでしょうが、既存のエンベディングデータを\\n新しいエンベディングデータに置き換えるときには、置き換えるべき既存のエンベディングデータの特定の\\nために、エンベディングベクトルの生の値を用いることは得策ではありません。代わりに、一意の識別子を\\n使うことが考えられます。\\n生成プロセスの実行\\nここからは、前のセクションでエンコードしたデータセットを使用して LLMからの応答を生成する方法を詳細に見て\\nいきます。\\n図5.クエリから応答を生成するワークフロー\\n13', metadata={'page': '12.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='図2.セマンティック情報を保持するベクトル演算の例\\n埋め込み（エンベディング）表現はベクトルであるため、ベクトル同士で算術演算を行うことができます。その際、その\\n演算結果のベクトルには、ベクトルの元となった自然言語の組み合わせに相当する意味概念が保持されることが期\\n待されます。\\nたとえば、 ベクトル同士の計算において、 《「王子」-「男性」+「女性」》 という計算式の結果として得\\nられるベクトルは、 「プリンセス」を表すベクトルと類似する値であると考えられます 。\\n同様に、《 「王子」-「男性」》 と《「プリンセス」 -「女性」》という２つの計算式の結果のベクトル\\nは、互いに非常に近い距離にあるでしょう。この場合、これらのベクトルは、「王または女王の直系の子孫である王\\n族（男女の別を問わない）」という概念を捉えていると見ることができます。\\n検索拡張生成（ RAG）パターン\\nLLMには、それが素のままで用いられた場合、生成された応答が特定のアプリケーションでは用をなさない可能性\\nがある、という限界があります。これは、 LLMに保存されている知識が通常そのモデルのトレーニングに用いられた\\nデータに限定されており、インターネット上で一般に入手できないような、最新のデータや、特定のドメインに関連す\\nるプロプライエタリデータが含まれていないことに由来します。\\n検索拡張生成（ RAG）の中心となるアイディアは、 LLMと、ユーザーが実行を依頼するタスクの解決に関連する独自\\nのドキュメントとを統合することです。\\n図3は、 RAGを用いたアプリケーションがどのように動作するかを示しています。まず、クエリがアプリケーションに対\\nしてサブミットされます。生成 AIアプリでは、クエリの表現は、検索エンジンタイプ、つまり「ジョージ・ワシントンの歴\\n史」というようなキーワード、または「ジョージ・ワシントンはいつ生まれたか」のような具体的な質問文、さらには\\n「ジョージ・ワシントンの生涯についてのエッセイを書け」という命令文、といったように様々な形式をとり得ます。独自\\nデータソースのエンベディングに使用したものと同じエンベディングモデルを使用してクエリのエンベディングを行い、\\nそのクエリのエンベディングを使用してベクトル検索を行うことによって、そのクエリに最も類似するデータを検索す\\nることができます。このようなデータ間の類似性に基づく効率的な検索は、ベクトル検索によって実現される重要な\\n機能です。\\n7', metadata={'page': '6.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='トを生成する LLM内で採用されているエンベディングモデルは、同じである必要はありません。また、ナレッ\\nジベース用のエンベディングモデルと LLMを、同じベンダー（またはオープンソースのモデルプロバイダー）\\nから選択する必要さえありません。ただし、ナレッジベースを構成するデータセットの作成と、そのデータ\\nセットに対するクエリの両方に、同じエンベディングモデルを用いることが重要です。\\nエンベディングモデルの選択を行う際に、様々なモデルの性能の違いについての見通しを得るために適し\\nた情報源として、「大規模テキストエンベディング・ベンチマーク（ Massive Text Embedding Benchmark,\\nMTEB）21」があります。ただし、このベンチマークを用いる際には、ベンチマークデータは定期的に再計算\\nされており、計算途中の場合には、完了するまでリーダーボードが利用できないことに注意してください。こ\\nのような時には、時間をおいて、もう一度確認してください。\\n取り敢えず始めてみたい場合には、 OpenAIのtext-embedding-ada-00222モデルは APIが公開されて\\nおり、性能も良好です。\\n自らホストすることのできるオープンソース技術を利用したい場合、 Instructor23モデルは優れた性能を備\\nえ、ユースケースに合わせてエンベディングを調整することもできます（ Instructorモデルには 3つのサイズ\\nがあり、小さい方の 2つのモデルは、ラップトップでも十分実行可能です）。\\n3.メタデータ利用の検討\\nベクトルデータそのものの利用だけでは十分でなく、ベクトルデータにメタデータを関連付けることが必要に\\nなる、いくつかの理由があります。\\nまず、ベクトル検索で一致する上位数件を取得した後、 LLMに渡すプロンプトでは、それらのエンベディン\\nグデータを、通常のテキストとして利用することになります。\\nエンベディングは通常、チャンクと呼ばれる文書全体よりも短いテキストのセクションに対して行われます。\\nユースケースやデータセットの種類によっては、各チャンクと一緒にドキュメントの全文を保存することが有\\n効な場合があります。この判断には、データセットそのものに関する知識が必要です。ドキュメントが非常に\\n長く、多くのトピックについて説明している場合は、チャンクのテキストを使用する方が、それぞれのチャンク\\nが別のクエリへ関連する可能性が高く、より合理的です。一方、文書がすべて特定のトピックに関するもの\\nであることがわかっている場合、完全な文書を利用する方が、 LLMにとって有益な詳細情報が含まれる可\\n能性があります。\\nその他の種類のメタデータとして、検討の価値があるものとして、そのデータを一意に識別するための IDが\\nあります。ドキュメント IDは、ソースドキュメントの変更によりエンベディングデータを更新する必要があるか\\nどうかを確認するときに利用できる場合があります。\\nベクトル検索を実行したいデータセットのサブセットを選択するためにフィルターできるメタデータを追加する\\nことも役立つ場合があります。簡単な例としては、「製品ドキュメント」や「社内 Wiki」など、さまざまなデータ\\nソースのカテゴリー情報を利用することが考えられます。例えば、「社内 Wiki」には特定のユーザーと共有\\nしたくないデータが含まれている場合があるため、そのソースを検索結果から除外する、といった風にこの\\n11', metadata={'page': '10.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='様々な場面で、このような体験を実現することは容易です。なぜなら、生成 AIモデルは、従来のチャットやカスタマー\\nサポートシステムとは異なり、事前定義された回答や構造化データベースに依存することがなく、質問のコンテキス\\nトとセマンティクスを理解した上で、あたかも人間のような応答をリアルタイムで生成できるからです。\\nこのパラダイムをさらに一歩進めて、上述のようなエクスペリエンスによって生成された応答の履歴をエンベディング\\nデータに変換した上で、将来の使用のために保存しておくことが考えられます。この追加機能は、 RAGアプリケー\\nションのためのデータプラットフォームを拡張して容易に実現可能です。この拡張は、単純なチャットボット履歴の保\\n存と比べて、はるかに強力です。なぜなら、すべての顧客とのやり取りの長期記憶が、完全に検索可能なものとして\\n構築され、将来の会話のコンテキストで参照できるようになるからです。\\nチャットやカスタマーサポートシステムは、先に説明した RAGデザインパターンを使用して LLMを用いて実装するこ\\nとができます。セマンティック検索の中心概念は、尋ねられた質問への回答に役立つドキュメントを見つけることで\\nす。これは、上述した検索の使用例ととても似通っています。ただし、チャットやカスタマーサポートシステムの場合\\nには、さらに、セマンティック検索の結果を使用してクエリに対するソリューションを提案するように LLMに指示する\\n後続の手順があります。この機能の組み合わせにより、エクスペリエンスははるかに汎用的になり、一様でない要求\\nを持つユーザーにとって魅力の大きいものになります。\\nまた、 LLMを用いて質問と回答のペアを生成することもでき、生成された質問と回答のペアをアプリケーションが\\nユーザーに提供する回答の精度向上のために活用することも考えられます。例えば、既存の文書から質問と回答\\nのペアを生成するよう LLMに指示することができます。あるいは、既存の質問と回答のペアを、異なる方向に拡張\\nすることもできるでしょう。また、既存のナレッジベースでカバーされていないトピック領域をカバーするより多くの質\\n問と回答のペアを含む合成データを生成することもできます。\\n分類\\n分類問題には多くのユースケースがあります。画像解析の分野は、その典型の一つです。例えば、写真に自動車が\\n写っているか、画像に信号機が含まれているか、といったことが問われます。\\nテキスト解析における分類問題として、ドキュメントのトピック分類があります。例えば、ドキュメントの内容がスポー\\nツ、政治、エンターテインメント等、どのジャンルに関するものであるかを判断します。\\n以下、 LLMを使用した分類に対するいくつかの異なるアプローチと、それぞれのメリットとデメリットについて解説し\\nます。まずは、実装の面で最も簡単なものから始め、より複雑なアプローチへと進んでいきます。\\n分類のための Few-shot学習\\nLLMは、 Few-shot学習プロンプトを用いて、テキスト分類のためにそのまま使用できます。 Few-shot学習のプロン\\nプトでは、分類用のさまざまなラベルの例を LLMに提供します。\\nポジティブな感情とネガティブな感情を分類する簡単な例を見てみましょう。ここでは、 OpenAIプラットフォーム 上\\nで、gpt-3.5-turboモデルを使用して、これがどのように可能になるかを例示します。\\nFew-shot学習の例を示すために、まず、プラットフォームが提供しているタスク定義機能の使用例を示します。\\n［システムプロンプト］\\n19', metadata={'page': '18.0', 'source': './DataStax_Vector-Search.pdf'}), Document(page_content='この例では、ベクトルは 2次元の情報を持ち、ベクトルデータを構成するそれぞれの要素は、次の２つの情報を表し\\nてます： [<衣服の種類 >, <服の色>]。ここで、「 赤いシャツ」のベクトルは [1, 1]で、「青いズボン」のベクトルは\\n[-1, -1]で表されているとします。そこに、「緑色のスカート」を加えます。スカートは機能的にはズボンに近く、緑は色\\nのスペクトルとして青に近いため、そのエンベディングベクトルは「青いズボン」に近い位置にあるはずで、例えば\\n[-0.8, -0.8] のような値を取ると推測することができます。\\n上の例は、エンベディングを使ってベクトル空間を構築する方法と、その空間の中で、オブジェクトが存在する場所\\nについてのアイディアを示しています。\\n次に、エンベディングに対する算術演算がどのように機能するかを示す例を見てみましょう。予め注意しておくと、ベ\\nクトル空間は実際には非常に大きな次元を持つ（ベクトルは多くの数値で構成される）のが一般的です。ここで扱う\\nベクトル空間は、「人物の肩書き」と「性別」という二つの概念を捉えた情報を扱っています。図 2を参照ください。\\n6', metadata={'page': '5.0', 'source': './DataStax_Vector-Search.pdf'})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## コスト計算"
      ],
      "metadata": {
        "id": "En-0MG5wA1eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "def ask(qa, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        answer = qa(query)\n",
        "\n",
        "    return answer, cb.total_cost"
      ],
      "metadata": {
        "id": "TVk-JG4z9VKW"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\":1})\n",
        "llm = ChatOpenAI(temperature=0, streaming=True)\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        #chain_type=\"map_rerank\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        verbose=True\n",
        "    )"
      ],
      "metadata": {
        "id": "hAbwb_9QBhEy"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer, cost = ask(qa,\"DATASTAXについて説明して\")\n",
        "print(answer)\n",
        "print(cost)"
      ],
      "metadata": {
        "id": "TjkW5XwI93XA",
        "outputId": "851b7e9f-3ba1-4328-8fd2-a5db973211b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'query': 'DATASTAXについて説明して', 'result': 'DataStaxは、分散データベース管理システムを提供する企業です。DataStaxの主力製品は、Apache Cassandraをベースにしたデータベース管理システムであり、大規模なデータセットを処理するための高い可用性と柔軟性を提供します。\\n\\nDataStaxのApache CassandraマネージドサービスであるAstraDBは、クラウド上でのデータベースの管理を簡素化し、スケーラビリティと信頼性を提供します。AstraDBは、ベクトル検索の機能も備えており、大規模なベクトルデータセットを操作する際にも優れた選択肢となります。\\n\\nDataStaxのデータベース管理システムは、ベクトルデータだけでなく、関連するメタデータも保存することができます。これにより、ベクトル検索だけでなく、メタデータを活用した操作も可能となります。\\n\\nDataStaxの製品は、大規模データの利用において信頼性とスケーラビリティを提供するだけでなく、汎用性も持っています。これは、AstraDBが分散データベース Apache Cassandraの拡張機能であるためです。', 'source_documents': [Document(page_content='カテゴリー情報を利用することができます。\\n4.エンベディングデータの生成と保存\\n先に、エンベディングに用いるためにモデルを選択しました。このモデルを用いてベクトルを生成することに\\nなります。\\nこのプロセスの最初のステップは、テキストのチャンク化です。これは、テキストをより短い文字列に分割す\\nる方法を決定するプロセスです。これは主に次の２つの理由から行われます。\\nまず、エンベディングモデルには入力として受け入れることのできるテキストの量に制限があるためです。\\ntext-ada-002 の場合、その制限は 8,192トークンです。受け入れられる文字列の長さがより短いモデルも\\nあります。\\nまた、チャンク化要否の判断は、モデルが持つ制限以外にも、データソースとアプリケーションの両方の面\\nからも検討する必要があります。アプリケーションがドキュメント全体をユーザーに返す必要がある場合、モ\\nデルが許す限り、できるだけ大きなチャンクを用いることは合理的です。あるいは、アプリケーションが、例\\nえば質疑応答システムの拡張といった場合のように、様々な事実に依存している場合、ドキュメント全体よ\\nりも、その中の数センテンス程度の短いチャンクの方が、一つのトピックに関する事実やアイデアを扱って\\nいる可能性が高く、このユースケースにより適しています。\\nさて、チャンクの長さが決まったら、次はチャンク化に関する戦略を立てる必要があります。これには多くの\\n方法があります。\\n最も簡単な方法は、テキストを均等な長さのチャンクに分割することです。この場合、重要な事実を説明し\\nている途中でチャンクが単語や文を分割してしまうことが生じる可能性があるため、チャンクのオーバーラッ\\nプを許容すると文書内の概念を適切に捉えるのに役立ちます。\\nより高度な、広く用いられている戦略として、ドキュメントの、または言語の構造に基づいたチャンク化があ\\nります。例としては、ドキュメントのセクション、段落ごと、またはセンテンスの境界を基準にしたチャンク化が\\n挙げられます。\\nドキュメントのチャンクのセットを作成したら、各チャンクのベクトルを、先の手順で定義した関連メタデータと\\n共に、データストアに保存します。これにより、近似最近傍探索（ ANN: Approximate Nearest Neighbor\\n）を利用した高速ベクトル検索が可能になります。 DataStaxの Apache Cassandraマネージドサービスで\\nあるAstraDB は、ベクトル検索の機能を有しており、大規模なベクトルデータセットを操作する場合の優れ\\nた選択肢となります。そしてまた、 AstraDBのベクトル検索機能は、高い可用性と柔軟なスケールを有した\\n分散データベース Apache Cassandraの拡張機能であるため、ベクトルとともに、そのベクトルに関連する\\nメタデータを保存しておくことも容易であり、ベクトル検索のみではなく、メタデータを使った操作との組み合\\nわせについても、利点を持っています。このような大規模データ利用のための信頼性・スケールと、データ\\nベースとしての汎用性は、 AstraDBの持つ、単なるベクトルデータベースとは異なる利点です。\\n12', metadata={'page': '11.0', 'source': './DataStax_Vector-Search.pdf'})]}\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qN2RLzPn-Wve"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}