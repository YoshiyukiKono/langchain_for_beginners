{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzhqgXXdbux3q1Hodc5v9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoshiyukiKono/langchain_for_beginners/blob/main/2b_chat-message-history_conversation-buffer-memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain ChatMessageHistory/ConversationBufferMemory with Astra DB\n",
        "\n",
        "LangChain: https://github.com/langchain-ai/langchain/releases/tag/v0.0.340\n",
        "\n"
      ],
      "metadata": {
        "id": "5GmUfFL0YCtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.3.5 tiktoken==0.5.1 cohere==4.36\n",
        "!pip install langchain==0.0.340"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMMMfJaKo4Ww",
        "outputId": "eda36a20-0b90-46c7-c70f-1c2bb3968f2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.3.5\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken==0.5.1\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere==4.36\n",
            "  Downloading cohere-4.36-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.3.5) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.3.5)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (4.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (3.8.6)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere==4.36)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere==4.36)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (6.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.5) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.3.5)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.5)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere==4.36) (3.17.0)\n",
            "Installing collected packages: h11, fastavro, backoff, tiktoken, httpcore, httpx, cohere, openai\n",
            "Successfully installed backoff-2.2.1 cohere-4.36 fastavro-1.8.2 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5 tiktoken-0.5.1\n",
            "Collecting langchain==0.0.340\n",
            "  Downloading langchain-0.0.340-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.340)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.340)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.340)\n",
            "  Downloading langsmith-0.0.67-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.340)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.340) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.340) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.340 langsmith-0.0.67 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cassandra-driver==3.28.0\n",
        "!pip install cassio==0.1.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k10ziT0zRYw",
        "outputId": "a0b14c47-4228-4365-8930-a13006c0af7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cassandra-driver==3.28.0\n",
            "  Downloading cassandra_driver-3.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver==3.28.0) (1.16.0)\n",
            "Collecting geomet<0.3,>=0.1 (from cassandra-driver==3.28.0)\n",
            "  Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver==3.28.0) (8.1.7)\n",
            "Installing collected packages: geomet, cassandra-driver\n",
            "Successfully installed cassandra-driver-3.28.0 geomet-0.2.1.post1\n",
            "Collecting cassio==0.1.3\n",
            "  Downloading cassio-0.1.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cassandra-driver>=3.28.0 in /usr/local/lib/python3.10/dist-packages (from cassio==0.1.3) (3.28.0)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from cassio==0.1.3) (1.23.5)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.10/dist-packages (from cassio==0.1.3) (2.31.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver>=3.28.0->cassio==0.1.3) (1.16.0)\n",
            "Requirement already satisfied: geomet<0.3,>=0.1 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver>=3.28.0->cassio==0.1.3) (0.2.1.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (2023.7.22)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver>=3.28.0->cassio==0.1.3) (8.1.7)\n",
            "Installing collected packages: cassio\n",
            "Successfully installed cassio-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Astra"
      ],
      "metadata": {
        "id": "gbIfQ5l_HaJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O secure-connect-demo.zip \"https://...\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig3ElhqbHd3G",
        "outputId": "cf33f74f-8741-4b43-e928-e0d581c64c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-01 05:03:54--  https://datastax-cluster-config-prod.s3.us-east-2.amazonaws.com/d5556151-ea9a-4309-8be3-b8ea2b1cd03d-1/secure-connect-demo.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA2AIQRQ76S2JCB77W%2F20230901%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20230901T050340Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=7db1257a20828e206535d85e46ffdfe26c4fe1166781a157033bbaac549b23c3\n",
            "Resolving datastax-cluster-config-prod.s3.us-east-2.amazonaws.com (datastax-cluster-config-prod.s3.us-east-2.amazonaws.com)... 3.5.131.128, 52.219.95.34, 52.219.179.34, ...\n",
            "Connecting to datastax-cluster-config-prod.s3.us-east-2.amazonaws.com (datastax-cluster-config-prod.s3.us-east-2.amazonaws.com)|3.5.131.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12247 (12K) [application/zip]\n",
            "Saving to: ‘secure-connect-demo.zip’\n",
            "\n",
            "secure-connect-demo 100%[===================>]  11.96K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-01 05:03:54 (119 MB/s) - ‘secure-connect-demo.zip’ saved [12247/12247]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SECURE_CONNECT_BUNDLE_PATH = 'secure-connect-demo.zip'"
      ],
      "metadata": {
        "id": "qGzwcnhfIROt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "ASTRA_CLIENT_ID = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVlMnfyJJVUb",
        "outputId": "f83cea73-a918-4262-b791-a597f474e282"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ASTRA_CLIENT_SECRET = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvKJFm2vJgoU",
        "outputId": "7dcf6fd4-b9be-40ea-d45d-0d92c0565a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "cloud_config= {\n",
        "  'secure_connect_bundle': SECURE_CONNECT_BUNDLE_PATH\n",
        "}\n",
        "auth_provider = PlainTextAuthProvider(ASTRA_CLIENT_ID, ASTRA_CLIENT_SECRET)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrGG5yAEJpnJ",
        "outputId": "5b9089c2-3677-4868-e1c8-cec028e729aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 216b7807-e6c4-471d-bb29-342090dcbade-us-east1.db.astra.datastax.com:29042:ee10308e-c37a-4011-b5df-46a93f3a5402. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 216b7807-e6c4-471d-bb29-342090dcbade-us-east1.db.astra.datastax.com:29042:ee10308e-c37a-4011-b5df-46a93f3a5402. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(137968592972064) 216b7807-e6c4-471d-bb29-342090dcbade-us-east1.db.astra.datastax.com:29042:ee10308e-c37a-4011-b5df-46a93f3a5402> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 216b7807-e6c4-471d-bb29-342090dcbade-us-east1.db.astra.datastax.com:29042:ee10308e-c37a-4011-b5df-46a93f3a5402. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0.11-13697dcfc157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_KEYSPACE = 'langchain'"
      ],
      "metadata": {
        "id": "7TxJnUlFJ4qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session.set_keyspace(YOUR_KEYSPACE)\n",
        "session"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im1ixFaIJ9Mw",
        "outputId": "e576b477-78c8-46af-cb4d-218709c4d7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cassandra.cluster.Session at 0x7d7b6f7a7370>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open AI\n",
        "\n",
        "### 動作確認"
      ],
      "metadata": {
        "id": "44jAlG1gYXMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxpCJvN4xACO",
        "outputId": "b86ae43b-2a98-404c-adb7-5e8b46c02b3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "client.models.list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48L2DDIrsoQV",
        "outputId": "9cfe5e67-bd22-4636-b2bc-ab804ea02c2c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncPage[Model](data=[Model(id='text-search-babbage-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'), Model(id='curie-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-003', created=1669599635, object='model', owned_by='openai-internal'), Model(id='text-search-babbage-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='babbage', created=1649358449, object='model', owned_by='openai'), Model(id='babbage-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-babbage-001', created=1649364043, object='model', owned_by='openai'), Model(id='text-similarity-davinci-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='davinci-similarity', created=1651172509, object='model', owned_by='openai-dev'), Model(id='code-davinci-edit-001', created=1649880484, object='model', owned_by='openai'), Model(id='curie-similarity', created=1651172510, object='model', owned_by='openai-dev'), Model(id='babbage-search-document', created=1651172510, object='model', owned_by='openai-dev'), Model(id='curie-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-ada-doc-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='davinci-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='text-similarity-babbage-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-search-davinci-doc-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai'), Model(id='babbage-similarity', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'), Model(id='davinci-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-similarity-curie-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='text-davinci-001', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-davinci-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-16k-0613', created=1685474247, object='model', owned_by='openai'), Model(id='ada-search-document', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-code-search-code', created=1651172505, object='model', owned_by='openai-dev'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='gpt-4-vision-preview', created=1698894917, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='gpt-4-0314', created=1687882410, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='davinci-search-document', created=1651172509, object='model', owned_by='openai-dev'), Model(id='curie-search-document', created=1651172508, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-code', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'), Model(id='text-search-ada-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='code-search-ada-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-text', created=1651172509, object='model', owned_by='openai-dev'), Model(id='code-search-babbage-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='ada-code-search-text', created=1651172510, object='model', owned_by='openai-dev'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='text-search-curie-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-002', created=1649880484, object='model', owned_by='openai'), Model(id='text-davinci-edit-001', created=1649809179, object='model', owned_by='openai'), Model(id='code-search-babbage-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='ada', created=1649357491, object='model', owned_by='openai'), Model(id='text-ada-001', created=1649364042, object='model', owned_by='openai'), Model(id='ada-similarity', created=1651172507, object='model', owned_by='openai-dev'), Model(id='code-search-ada-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='text-similarity-ada-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'), Model(id='canary-whisper', created=1699656801, object='model', owned_by='system'), Model(id='text-search-curie-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='text-curie-001', created=1649364043, object='model', owned_by='openai'), Model(id='curie', created=1649359874, object='model', owned_by='openai'), Model(id='canary-tts', created=1699492935, object='model', owned_by='system'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'), Model(id='davinci', created=1649359874, object='model', owned_by='openai'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system')], object='list')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### デバッグログレベル設定"
      ],
      "metadata": {
        "id": "_r3q523qFiC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger('openai').setLevel(logging.DEBUG)"
      ],
      "metadata": {
        "id": "jAX6YYAeFlkR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Message\n",
        "\n",
        "https://python.langchain.com/docs/integrations/memory/aws_dynamodb\n",
        "\n",
        "https://python.langchain.com/docs/integrations/memory/cassandra_chat_message_history\n"
      ],
      "metadata": {
        "id": "n5Nxf62MNCPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import CassandraChatMessageHistory\n",
        "\n",
        "message_history = CassandraChatMessageHistory(\n",
        "    session_id=\"test-session\",\n",
        "    session=session,\n",
        "    keyspace=YOUR_KEYSPACE,\n",
        ")"
      ],
      "metadata": {
        "id": "1YUyXG0sNflJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\", chat_memory=message_history, return_messages=True\n",
        ")"
      ],
      "metadata": {
        "id": "JyH6J5XdNwe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python_repl = PythonREPL()\n",
        "\n",
        "# You can create the tool to pass to an agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"python_repl\",\n",
        "        description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "        func=python_repl.run,\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "ZcqZmdEfN0X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0)\n",
        "agent_chain = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "qeOUj5BoN37V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"Hello!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jB17TW2dN64J",
        "outputId": "46b08732-891b-4d0b-81b0-2a4a07e144eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'My name is Bob.'}, {'role': 'assistant', 'content': 'Nice to meet you, Bob! How can I assist you today?'}, {'role': 'user', 'content': 'Who am I?'}, {'role': 'assistant', 'content': \"As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\"}, {'role': 'user', 'content': 'TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nHello!'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stop': ['\\nObservation:', '\\n\\tObservation:'], 'stream': False, 'temperature': 0.0}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: My name is Bob.\\nAI: Nice to meet you, Bob! How can I assist you today?\\nHuman: Who am I?\\nAI: As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\\nHuman: TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \\\"action_input\\\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER'S INPUT\\n--------------------\\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nHello!\"\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [4.04s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Hello! How can I assist you today?\\\"\\n}\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Hello! How can I assist you today?\\\"\\n}\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 25,\n",
            "      \"prompt_tokens\": 668,\n",
            "      \"total_tokens\": 693\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [4.05s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Hello! How can I assist you today?\\\"\\n}\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [4.05s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Hello! How can I assist you today?\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"Who owns Twitter?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-Ix_m-xPN94d",
        "outputId": "df685da5-b8d5-45eb-9d69-c68d447403a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'My name is Bob.'}, {'role': 'assistant', 'content': 'Nice to meet you, Bob! How can I assist you today?'}, {'role': 'user', 'content': 'Who am I?'}, {'role': 'assistant', 'content': \"As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\"}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWho owns Twitter?'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stop': ['\\nObservation:', '\\n\\tObservation:'], 'stream': False, 'temperature': 0.0}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: My name is Bob.\\nAI: Nice to meet you, Bob! How can I assist you today?\\nHuman: Who am I?\\nAI: As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \\\"action_input\\\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER'S INPUT\\n--------------------\\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWho owns Twitter?\"\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [2.67s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"To find out who owns Twitter, you can use a search engine or visit a reliable news source.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"To find out who owns Twitter, you can use a search engine or visit a reliable news source.\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 20,\n",
            "      \"prompt_tokens\": 689,\n",
            "      \"total_tokens\": 709\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [2.67s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"To find out who owns Twitter, you can use a search engine or visit a reliable news source.\"\n",
            "}\n",
            "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [2.67s] Chain run errored with error:\n",
            "\u001b[0m\"ValueError('An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: To find out who owns Twitter, you can use a search engine or visit a reliable news source.')\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/conversational_chat/output_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# stored as markdown)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_json_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Parse the JSON string into a Python dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;31m# Call the LLM to see what to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             output = self.agent.plan(\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0mintermediate_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36mplan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0mfull_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/conversational_chat/output_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# OutputParserException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not parse LLM output: {text}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: To find out who owns Twitter, you can use a search engine or visit a reliable news source.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-79128a3fa82f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Who owns Twitter?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    511\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs = (\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0;31m# We now enter the agent loop (until it returns something).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1245\u001b[0;31m             next_step_output = self._take_next_step(\n\u001b[0m\u001b[1;32m   1246\u001b[0m                 \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m                 \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0mraise_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1044\u001b[0m                     \u001b[0;34m\"An output parsing error occurred. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0;34m\"In order to pass this error back to the agent and have it try \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: To find out who owns Twitter, you can use a search engine or visit a reliable news source."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0)\n",
        "agent_chain = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        "    handle_parsing_errors=True\n",
        ")"
      ],
      "metadata": {
        "id": "zPmgY1RIN3bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"Who owns Twitter?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7UKaZM5COM0g",
        "outputId": "f7c9bc3d-f3ba-4bfc-aad9-ef0f08f4ee27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'My name is Bob.'}, {'role': 'assistant', 'content': 'Nice to meet you, Bob! How can I assist you today?'}, {'role': 'user', 'content': 'Who am I?'}, {'role': 'assistant', 'content': \"As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\"}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWho owns Twitter?'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stop': ['\\nObservation:', '\\n\\tObservation:'], 'stream': False, 'temperature': 0.0}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: My name is Bob.\\nAI: Nice to meet you, Bob! How can I assist you today?\\nHuman: Who am I?\\nAI: As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \\\"action_input\\\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER'S INPUT\\n--------------------\\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWho owns Twitter?\"\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
            "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'My name is Bob.'}, {'role': 'assistant', 'content': 'Nice to meet you, Bob! How can I assist you today?'}, {'role': 'user', 'content': 'Who am I?'}, {'role': 'assistant', 'content': \"As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\"}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWho owns Twitter?'}, {'role': 'assistant', 'content': 'Could not parse LLM output: To find out who owns Twitter, you can use a search engine or visit a reliable news source.'}, {'role': 'user', 'content': \"TOOL RESPONSE: \\n---------------------\\nInvalid or incomplete response\\n\\nUSER'S INPUT\\n--------------------\\n\\nOkay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else.\"}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stop': ['\\nObservation:', '\\n\\tObservation:'], 'stream': False, 'temperature': 0.0}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [2.62s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"To find out who owns Twitter, you can use a search engine or visit a reliable news source.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"To find out who owns Twitter, you can use a search engine or visit a reliable news source.\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 20,\n",
            "      \"prompt_tokens\": 689,\n",
            "      \"total_tokens\": 709\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [2.63s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"To find out who owns Twitter, you can use a search engine or visit a reliable news source.\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:_Exception] Entering Tool run with input:\n",
            "\u001b[0m\"Invalid or incomplete response\"\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:_Exception] [3ms] Exiting Tool run with output:\n",
            "\u001b[0m\"Invalid or incomplete response\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: My name is Bob.\\nAI: Nice to meet you, Bob! How can I assist you today?\\nHuman: Who am I?\\nAI: As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \\\"action_input\\\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER'S INPUT\\n--------------------\\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWho owns Twitter?\\nAI: Could not parse LLM output: To find out who owns Twitter, you can use a search engine or visit a reliable news source.\\nHuman: TOOL RESPONSE: \\n---------------------\\nInvalid or incomplete response\\n\\nUSER'S INPUT\\n--------------------\\n\\nOkay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else.\"\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] [3.70s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Twitter is a publicly traded company, so it is owned by its shareholders who hold its stock.\\\"\\n}\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Twitter is a publicly traded company, so it is owned by its shareholders who hold its stock.\\\"\\n}\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 35,\n",
            "      \"prompt_tokens\": 801,\n",
            "      \"total_tokens\": 836\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] [3.70s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Twitter is a publicly traded company, so it is owned by its shareholders who hold its stock.\\\"\\n}\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [6.33s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Twitter is a publicly traded company, so it is owned by its shareholders who hold its stock.\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Twitter is a publicly traded company, so it is owned by its shareholders who hold its stock.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"My name is Bob.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ivSkhZoYOTGV",
        "outputId": "01d4d0a7-95c4-42eb-e048-6707d563b2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'My name is Bob.'}, {'role': 'assistant', 'content': 'Nice to meet you, Bob! How can I assist you today?'}, {'role': 'user', 'content': 'Who am I?'}, {'role': 'assistant', 'content': \"As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\"}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Who owns Twitter?'}, {'role': 'assistant', 'content': 'Twitter is a publicly traded company, so it is owned by its shareholders who hold its stock.'}, {'role': 'user', 'content': 'TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nMy name is Bob.'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stop': ['\\nObservation:', '\\n\\tObservation:'], 'stream': False, 'temperature': 0.0}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: My name is Bob.\\nAI: Nice to meet you, Bob! How can I assist you today?\\nHuman: Who am I?\\nAI: As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: Who owns Twitter?\\nAI: Twitter is a publicly traded company, so it is owned by its shareholders who hold its stock.\\nHuman: TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \\\"action_input\\\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER'S INPUT\\n--------------------\\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nMy name is Bob.\"\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [2.43s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Nice to meet you, Bob!\\\"\\n}\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Nice to meet you, Bob!\\\"\\n}\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 23,\n",
            "      \"prompt_tokens\": 721,\n",
            "      \"total_tokens\": 744\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [2.43s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"Nice to meet you, Bob!\\\"\\n}\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [2.43s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Nice to meet you, Bob!\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nice to meet you, Bob!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"Who am I?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pu5WD6xDOmFQ",
        "outputId": "bf6717a5-1ae6-49f0-87c6-af36ee6b74b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'My name is Bob.'}, {'role': 'assistant', 'content': 'Nice to meet you, Bob! How can I assist you today?'}, {'role': 'user', 'content': 'Who am I?'}, {'role': 'assistant', 'content': \"As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\"}, {'role': 'user', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'Who owns Twitter?'}, {'role': 'assistant', 'content': 'Twitter is a publicly traded company, so it is owned by its shareholders who hold its stock.'}, {'role': 'user', 'content': 'My name is Bob.'}, {'role': 'assistant', 'content': 'Nice to meet you, Bob!'}, {'role': 'user', 'content': 'TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \"action_input\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWho am I?'}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stop': ['\\nObservation:', '\\n\\tObservation:'], 'stream': False, 'temperature': 0.0}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: My name is Bob.\\nAI: Nice to meet you, Bob! How can I assist you today?\\nHuman: Who am I?\\nAI: As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: Who owns Twitter?\\nAI: Twitter is a publicly traded company, so it is owned by its shareholders who hold its stock.\\nHuman: My name is Bob.\\nAI: Nice to meet you, Bob!\\nHuman: TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \\\"action_input\\\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER'S INPUT\\n--------------------\\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWho am I?\"\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [2.98s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"You are Bob.\\\"\\n}\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"You are Bob.\\\"\\n}\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 20,\n",
            "      \"prompt_tokens\": 740,\n",
            "      \"total_tokens\": 760\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [2.98s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": \\\"You are Bob.\\\"\\n}\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [2.99s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"You are Bob.\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are Bob.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"prompts\": [\n",
        "    \"System: Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: My name is Bob.\\nAI: Nice to meet you, Bob! How can I assist you today?\\nHuman: Who am I?\\nAI: As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\\nHuman: Hello!\\nAI: Hello! How can I assist you today?\\nHuman: Who owns Twitter?\\nAI: Twitter is a publicly traded company, so it is owned by its shareholders who hold its stock.\\nHuman: My name is Bob.\\nAI: Nice to meet you, Bob!\\nHuman: TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> python_repl: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": string, \\\\\\\\ The action to take. Must be one of python_repl\\n    \\\"action_input\\\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER'S INPUT\\n--------------------\\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nWho am I?\"\n",
        "  ]\n"
      ],
      "metadata": {
        "id": "kVbDBssvOv6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "token@cqlsh:langchain> select * from message_store;\n",
        "\n",
        " partition_id | row_id                               | body_blob\n",
        "--------------+--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        " test-session | 1d6ee1c4-8a99-11ee-b674-0242ac1c000c | {\"type\": \"ai\", \"data\": {\"content\": \"As an AI language model, I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality. My primary function is to provide information and answer questions to the best of my knowledge and abilities. If you have any concerns about privacy or data security, please let me know, and I will do my best to address them. How can I assist you today?\", \"additional_kwargs\": {}, \"type\": \"ai\", \"example\": false}}\n",
        " test-session | 1d6c4d10-8a99-11ee-b674-0242ac1c000c |                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {\"type\": \"human\", \"data\": {\"content\": \"Who am I?\", \"additional_kwargs\": {}, \"type\": \"human\", \"example\": false}}\n",
        " test-session | 09111468-8a99-11ee-b674-0242ac1c000c |                                                                                                                                                                                                                                                                                                                                                                                                                                   {\"type\": \"ai\", \"data\": {\"content\": \"Nice to meet you, Bob! How can I assist you today?\", \"additional_kwargs\": {}, \"type\": \"ai\", \"example\": false}}\n",
        " test-session | 090e9f44-8a99-11ee-b674-0242ac1c000c |                                                                                                                                                                                                                                                                                                                                                                                                                                                                {\"type\": \"human\", \"data\": {\"content\": \"My name is Bob.\", \"additional_kwargs\": {}, \"type\": \"human\", \"example\": false}}\n",
        " test-session | 02645166-8a99-11ee-b674-0242ac1c000c |                                                                                                                                                                                                                                                                                                                                                                                                                                                   {\"type\": \"ai\", \"data\": {\"content\": \"Hello! How can I assist you today?\", \"additional_kwargs\": {}, \"type\": \"ai\", \"example\": false}}\n",
        " test-session | 0261a164-8a99-11ee-b674-0242ac1c000c |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {\"type\": \"human\", \"data\": {\"content\": \"Hello!\", \"additional_kwargs\": {}, \"type\": \"human\", \"example\": false}}\n",
        " test-session | bc61b302-8a98-11ee-b674-0242ac1c000c |                                                                                                                                                                                                                                                                                                                                                                                                                                                   {\"type\": \"ai\", \"data\": {\"content\": \"Hello! How can I assist you today?\", \"additional_kwargs\": {}, \"type\": \"ai\", \"example\": false}}\n",
        " test-session | bc56e83c-8a98-11ee-b674-0242ac1c000c |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {\"type\": \"human\", \"data\": {\"content\": \"Hello!\", \"additional_kwargs\": {}, \"type\": \"human\", \"example\": false}}\n",
        "\n",
        "(8 rows)\n",
        "token@cqlsh:langchain>\n",
        "```"
      ],
      "metadata": {
        "id": "tW-ate8XNGwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "(8 rows)\n",
        "token@cqlsh:langchain> desc message_store;\n",
        "\n",
        "CREATE TABLE langchain.message_store (\n",
        "    partition_id text,\n",
        "    row_id timeuuid,\n",
        "    body_blob text,\n",
        "    PRIMARY KEY (partition_id, row_id)\n",
        ") WITH CLUSTERING ORDER BY (row_id DESC)\n",
        "    AND additional_write_policy = '99p'\n",
        "    AND bloom_filter_fp_chance = 0.01\n",
        "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
        "    AND comment = ''\n",
        "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy'}\n",
        "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
        "    AND crc_check_chance = 1.0\n",
        "    AND default_time_to_live = 0\n",
        "    AND gc_grace_seconds = 864000\n",
        "    AND max_index_interval = 2048\n",
        "    AND memtable_flush_period_in_ms = 0\n",
        "    AND min_index_interval = 128\n",
        "    AND read_repair = 'BLOCKING'\n",
        "    AND speculative_retry = '99p';\n",
        "```"
      ],
      "metadata": {
        "id": "HIaDgnQPRHQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "token@cqlsh:langchain> ALTER TABLE langchain.message_store WITH default_time_to_live = 3600;\n",
        "\n",
        "token@cqlsh:langchain> desc message_store;\n",
        "\n",
        "CREATE TABLE langchain.message_store (\n",
        "    partition_id text,\n",
        "    row_id timeuuid,\n",
        "    body_blob text,\n",
        "    PRIMARY KEY (partition_id, row_id)\n",
        ") WITH CLUSTERING ORDER BY (row_id DESC)\n",
        "    AND additional_write_policy = '99p'\n",
        "    AND bloom_filter_fp_chance = 0.01\n",
        "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
        "    AND comment = ''\n",
        "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy'}\n",
        "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
        "    AND crc_check_chance = 1.0\n",
        "    AND default_time_to_live = 3600\n",
        "    AND gc_grace_seconds = 864000\n",
        "    AND max_index_interval = 2048\n",
        "    AND memtable_flush_period_in_ms = 0\n",
        "    AND min_index_interval = 128\n",
        "    AND read_repair = 'BLOCKING'\n",
        "    AND speculative_retry = '99p';\n",
        "\n",
        "token@cqlsh:langchain>\n",
        "```"
      ],
      "metadata": {
        "id": "E9MhNIlZSxBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 様々なメモリー"
      ],
      "metadata": {
        "id": "LHMemdiMYQza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.memory import VectorStoreRetrieverMemory"
      ],
      "metadata": {
        "id": "KKY6a3_JUlfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"another_chat_history\", chat_memory=message_history, return_messages=True\n",
        ")"
      ],
      "metadata": {
        "id": "n4DcMJe9YmKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryMemory(\n",
        "    memory_key=\"another_chat_history\", chat_memory=message_history, return_messages=True, llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "MMMsA9Z4Vam-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(\n",
        "    memory_key=\"another_chat_history\", chat_memory=message_history, return_messages=True, llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "ugLpeZSaVeGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(\n",
        "    memory_key=\"another_chat_history\", chat_memory=message_history, return_messages=True, llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "x7BnhjPSVAVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IFgImVZYVkLR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}