{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoshiyukiKono/langchain_for_beginners/blob/main/1_RAG_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDFファイルを使ったRAG\n",
        "\n",
        "https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf\n",
        "\n",
        "## 準備\n",
        "\n",
        "### Astra DBの準備\n",
        " 1. Astraで、新しいベクトル検索対応データベース(`demo`)を作成します（データベース名はConnect Bundleファイルに関係します）。\n",
        " 2. キースペース (`langchain`)を作成します（キースペース名はデータベース作成時のダイアログにて指定します）\n",
        " 3. `Database Administrator`権限を持つトークンを生成します。\n",
        "\n",
        " 上記で作成済みのデータベース/キースペースに、このチュートリアルの中で、テーブルとインデックスを作成します。\n",
        "\n",
        "\n",
        "### パッケージインストール"
      ],
      "metadata": {
        "id": "YODRRq-0Y4Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.3.5 tiktoken==0.5.1 cohere==4.36\n",
        "!pip install langchain==0.0.340"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMMMfJaKo4Ww",
        "outputId": "9bb7d268-25f2-4c7a-8060-0d2ee2b2c545"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.3.5\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken==0.5.1\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere==4.36\n",
            "  Downloading cohere-4.36-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.3.5) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.3.5)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (4.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (3.8.6)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere==4.36)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere==4.36)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (6.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.5) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.3.5)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.5)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere==4.36) (3.17.0)\n",
            "Installing collected packages: h11, fastavro, backoff, tiktoken, httpcore, httpx, cohere, openai\n",
            "Successfully installed backoff-2.2.1 cohere-4.36 fastavro-1.8.2 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5 tiktoken-0.5.1\n",
            "Collecting langchain==0.0.340\n",
            "  Downloading langchain-0.0.340-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.340)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.340)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.340)\n",
            "  Downloading langsmith-0.0.67-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.340)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.340) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.340) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.340 langsmith-0.0.67 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cassandra-driver==3.28.0 cassio==0.1.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k10ziT0zRYw",
        "outputId": "f0d8a1b2-4787-4052-9ee4-c2c4a649537a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cassandra-driver==3.28.0\n",
            "  Downloading cassandra_driver-3.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cassio==0.1.3\n",
            "  Downloading cassio-0.1.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver==3.28.0) (1.16.0)\n",
            "Collecting geomet<0.3,>=0.1 (from cassandra-driver==3.28.0)\n",
            "  Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from cassio==0.1.3) (1.23.5)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.10/dist-packages (from cassio==0.1.3) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver==3.28.0) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (2023.7.22)\n",
            "Installing collected packages: geomet, cassandra-driver, cassio\n",
            "Successfully installed cassandra-driver-3.28.0 cassio-0.1.3 geomet-0.2.1.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf==3.17.0"
      ],
      "metadata": {
        "id": "oX33RRkrY2WY",
        "outputId": "eb94589c-12d8-4d39-ed59-818b54b9e9bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf==3.17.0\n",
            "  Downloading pypdf-3.17.0-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI 接続"
      ],
      "metadata": {
        "id": "IAAChGc-tpsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "WLkDfg9_dB_l",
        "outputId": "cf9d1e27-2c2d-4350-dcdf-ae05f3061cad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "入力されたキーによる接続確認"
      ],
      "metadata": {
        "id": "qX3AzbOTGwps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "client.models.list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGmRlMyBtZ4n",
        "outputId": "3ab864dc-a623-47ca-ca80-5e7c0db50851"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncPage[Model](data=[Model(id='text-search-babbage-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'), Model(id='curie-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-003', created=1669599635, object='model', owned_by='openai-internal'), Model(id='text-search-babbage-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='babbage', created=1649358449, object='model', owned_by='openai'), Model(id='babbage-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-babbage-001', created=1649364043, object='model', owned_by='openai'), Model(id='text-similarity-davinci-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='davinci-similarity', created=1651172509, object='model', owned_by='openai-dev'), Model(id='code-davinci-edit-001', created=1649880484, object='model', owned_by='openai'), Model(id='curie-similarity', created=1651172510, object='model', owned_by='openai-dev'), Model(id='babbage-search-document', created=1651172510, object='model', owned_by='openai-dev'), Model(id='curie-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-ada-doc-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='davinci-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='text-similarity-babbage-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-search-davinci-doc-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai'), Model(id='babbage-similarity', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'), Model(id='davinci-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-similarity-curie-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='text-davinci-001', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-davinci-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-16k-0613', created=1685474247, object='model', owned_by='openai'), Model(id='ada-search-document', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-code-search-code', created=1651172505, object='model', owned_by='openai-dev'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='gpt-4-vision-preview', created=1698894917, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='gpt-4-0314', created=1687882410, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='davinci-search-document', created=1651172509, object='model', owned_by='openai-dev'), Model(id='curie-search-document', created=1651172508, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-code', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'), Model(id='text-search-ada-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='code-search-ada-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-text', created=1651172509, object='model', owned_by='openai-dev'), Model(id='code-search-babbage-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='ada-code-search-text', created=1651172510, object='model', owned_by='openai-dev'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='text-search-curie-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-002', created=1649880484, object='model', owned_by='openai'), Model(id='text-davinci-edit-001', created=1649809179, object='model', owned_by='openai'), Model(id='code-search-babbage-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada', created=1649357491, object='model', owned_by='openai'), Model(id='text-ada-001', created=1649364042, object='model', owned_by='openai'), Model(id='ada-similarity', created=1651172507, object='model', owned_by='openai-dev'), Model(id='code-search-ada-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='text-similarity-ada-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'), Model(id='canary-whisper', created=1699656801, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='text-search-curie-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='text-curie-001', created=1649364043, object='model', owned_by='openai'), Model(id='curie', created=1649359874, object='model', owned_by='openai'), Model(id='canary-tts', created=1699492935, object='model', owned_by='system'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'), Model(id='davinci', created=1649359874, object='model', owned_by='openai'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system')], object='list')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Astra DB接続\n",
        "\n",
        "Connect Bundleファイルを実行環境に配置します。\n",
        "\n",
        "Connect Bundleファイルを Astra から Colab 環境に直接ダウンロードすることもできます。 あなたのAstra DB環境からコピーした情報で下のセルを変更してください。「WGET」タブからコマンド全体をコピーすることができるため、ノートブックで実行する際に、先頭に`!`を補って、そのまま用いることができます。\n",
        "\n",
        "ただし、このAstraコントロールプレーンに表示されるURL は静的ではないことに注意が必要です。そのため、後日、再度実行する場合には、URL を再度コピーし直す必要があります。\n",
        "\n",
        "あるいは、AstraDBコントロールプレーンからダウンロードしたファイルを左サイドメニュー「Files」からアップロードしてください。"
      ],
      "metadata": {
        "id": "5GmUfFL0YCtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O secure-connect-demo.zip \"https://datastax-cluster...\""
      ],
      "metadata": {
        "id": "IUm51RWBJiU9",
        "outputId": "2fc902e0-b671-4ee1-847b-5c0ff68c82c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-01 02:03:42--  https://datastax-cluster-config-prod.s3.us-east-2.amazonaws.com/2c170f07-40f1-4550-b361-6b2c8dabb94f-1/secure-connect-demo.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA2AIQRQ76S2JCB77W%2F20231201%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20231201T020327Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=d74aced339f0ceac09b701bfa5a3b48f2227d2e3d9af4ace0465873a9b02ab90\n",
            "Resolving datastax-cluster-config-prod.s3.us-east-2.amazonaws.com (datastax-cluster-config-prod.s3.us-east-2.amazonaws.com)... 3.5.128.147, 52.219.104.16, 52.219.141.2, ...\n",
            "Connecting to datastax-cluster-config-prod.s3.us-east-2.amazonaws.com (datastax-cluster-config-prod.s3.us-east-2.amazonaws.com)|3.5.128.147|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12254 (12K) [application/zip]\n",
            "Saving to: ‘secure-connect-demo.zip’\n",
            "\n",
            "secure-connect-demo 100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-12-01 02:03:43 (113 MB/s) - ‘secure-connect-demo.zip’ saved [12254/12254]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "データベース名が違う場合は、以下の名称(`demo`部分)を変更する必要があります。"
      ],
      "metadata": {
        "id": "mB3G1jkvHmGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SECURE_CONNECT_BUNDLE_PATH = 'secure-connect-demo.zip'"
      ],
      "metadata": {
        "id": "qGzwcnhfIROt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "ASTRA_CLIENT_ID = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVlMnfyJJVUb",
        "outputId": "e6f77287-6696-4689-ba41-27847f976107"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ASTRA_CLIENT_SECRET = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvKJFm2vJgoU",
        "outputId": "dec362a0-4154-4573-b5ad-1d2a916dac23"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "接続確認"
      ],
      "metadata": {
        "id": "LQ6R6pgsH90r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "cloud_config= {\n",
        "  'secure_connect_bundle': SECURE_CONNECT_BUNDLE_PATH\n",
        "}\n",
        "auth_provider = PlainTextAuthProvider(ASTRA_CLIENT_ID, ASTRA_CLIENT_SECRET)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrGG5yAEJpnJ",
        "outputId": "cb20bb0d-e66e-4d46-fcba-42d4bfdf6e28"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:dfda82ab-0ac8-45a2-922e-aa871d59a554. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:dfda82ab-0ac8-45a2-922e-aa871d59a554. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(132424679355200) 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:dfda82ab-0ac8-45a2-922e-aa871d59a554> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:dfda82ab-0ac8-45a2-922e-aa871d59a554. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0.11-13697dcfc157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChainの利用\n",
        "\n"
      ],
      "metadata": {
        "id": "ZlSnousTt2cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Cassandra\n",
        "from langchain.document_loaders import TextLoader"
      ],
      "metadata": {
        "id": "FCsCyZLJcTgF"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDFファイルの準備\n",
        "\n",
        "このリポジトリの`data`フォルダにあるファイル(`DataStax_Vector-Search.pdf`)を、Colab環境にアップロードしてください。\n",
        "\n",
        "このアップロードしたファイルを下記で読み込みます。"
      ],
      "metadata": {
        "id": "erSlFwzJo71s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"./DataStax_Vector-Search.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "uqx1XkeMZCMd"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pages))\n",
        "print(pages[0])\n",
        "print(pages[len(pages)-1])"
      ],
      "metadata": {
        "id": "QzK8Pk4RZY5I",
        "outputId": "ef5fdc2b-8c57-4e14-ef5e-2b8fe3614a76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "page_content='ホワイトペーパー\\n生成 AIアプリ\\nのための\\nベクトル検索\\nAIアプリケーション開発にベクトル検索を活用す\\nるための開発者/アーキテクト向けガイド\\nこのドキュメントは、生成 AIアプリケーションを企業独\\n自のデータと組み合わせて、設計・構築しようとしてい\\nる全ての方のためのガイドです。組織が理解すべき重\\n要な概念と考慮事項を取り扱うだけでなく、ベクトル検\\n索を用いて、 LLMの持つ機能を大幅に拡張するため\\nのシンプルで強力なアプローチについても解説しま\\nす。' metadata={'source': './DataStax_Vector-Search.pdf', 'page': 0}\n",
            "page_content='20. Karpathy, Andrej. “Andrej Karpathy on Twitter: \"The hottest new programming language is\\nEnglish.\"” Twitter , 2023年 1月24日 ,https://twitter .com/karpathy/status/1617979122625712128\\n21. “MTEB Leaderboard - a Hugging Face Space by mteb.” Hugging Face,\\nhttps://huggingface.co/spaces/mteb/leaderboard\\n22.https://platform.openai.com/docs/guides/embeddings/types-of-embedding-models\\n23. Instructor Text Embedding, https://instructor-embedding.github.io/\\n24. “Libraries - OpenAI API.” Platform OpenAI, https://platform.openai.com/docs/libraries\\n25. “Vertex AI.” Google Cloud, https://cloud.google.com/vertex-ai\\n©2023 DataStax Inc.、全著作権所有。 DataStax は、米国およびその他の国における DataStax, Inc. およびそ\\nの子会社の登録商標です。\\n24' metadata={'source': './DataStax_Vector-Search.pdf', 'page': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "Xv2mLw5ece87"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))\n",
        "print(docs[0])\n",
        "print(docs[len(docs)-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpGh_t-UuD9x",
        "outputId": "48253957-6416-4a94-980b-ecd010f83762"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "page_content='ホワイトペーパー\\n生成 AIアプリ\\nのための\\nベクトル検索\\nAIアプリケーション開発にベクトル検索を活用す\\nるための開発者/アーキテクト向けガイド\\nこのドキュメントは、生成 AIアプリケーションを企業独\\n自のデータと組み合わせて、設計・構築しようとしてい\\nる全ての方のためのガイドです。組織が理解すべき重\\n要な概念と考慮事項を取り扱うだけでなく、ベクトル検\\n索を用いて、 LLMの持つ機能を大幅に拡張するため\\nのシンプルで強力なアプローチについても解説しま\\nす。' metadata={'source': './DataStax_Vector-Search.pdf', 'page': 0}\n",
            "page_content='20. Karpathy, Andrej. “Andrej Karpathy on Twitter: \"The hottest new programming language is\\nEnglish.\"” Twitter , 2023年 1月24日 ,https://twitter .com/karpathy/status/1617979122625712128\\n21. “MTEB Leaderboard - a Hugging Face Space by mteb.” Hugging Face,\\nhttps://huggingface.co/spaces/mteb/leaderboard\\n22.https://platform.openai.com/docs/guides/embeddings/types-of-embedding-models\\n23. Instructor Text Embedding, https://instructor-embedding.github.io/\\n24. “Libraries - OpenAI API.” Platform OpenAI, https://platform.openai.com/docs/libraries\\n25. “Vertex AI.” Google Cloud, https://cloud.google.com/vertex-ai\\n©2023 DataStax Inc.、全著作権所有。 DataStax は、米国およびその他の国における DataStax, Inc. およびそ\\nの子会社の登録商標です。\\n24' metadata={'source': './DataStax_Vector-Search.pdf', 'page': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "より、細かいチャンクに分割してみる。"
      ],
      "metadata": {
        "id": "azkKY53iKZ_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=0)\n",
        "\n",
        "texts = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "juTkrIesJInq"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts))\n",
        "print(texts[0])\n",
        "print(texts[len(texts)-1])"
      ],
      "metadata": {
        "id": "VpJErq2BJLo4",
        "outputId": "7af3499c-c31a-491a-a5ee-e7c682999ae7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82\n",
            "page_content='ホワイトペーパー\\n生成 AIアプリ\\nのための\\nベクトル検索\\nAIアプリケーション開発にベクトル検索を活用す\\nるための開発者/アーキテクト向けガイド\\nこのドキュメントは、生成 AIアプリケーションを企業独\\n自のデータと組み合わせて、設計・構築しようとしてい\\nる全ての方のためのガイドです。組織が理解すべき重\\n要な概念と考慮事項を取り扱うだけでなく、ベクトル検\\n索を用いて、 LLMの持つ機能を大幅に拡張するため\\nのシンプルで強力なアプローチについても解説しま\\nす。' metadata={'source': './DataStax_Vector-Search.pdf', 'page': 0}\n",
            "page_content='23. Instructor Text Embedding, https://instructor-embedding.github.io/\\n24. “Libraries - OpenAI API.” Platform OpenAI, https://platform.openai.com/docs/libraries\\n25. “Vertex AI.” Google Cloud, https://cloud.google.com/vertex-ai\\n©2023 DataStax Inc.、全著作権所有。 DataStax は、米国およびその他の国における DataStax, Inc. およびそ\\nの子会社の登録商標です。\\n24' metadata={'source': './DataStax_Vector-Search.pdf', 'page': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents=texts"
      ],
      "metadata": {
        "id": "dlwT8kIUKlkG"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ベクトルストアの構築"
      ],
      "metadata": {
        "id": "lWMh_qez4mGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_KEYSPACE = 'langchain'\n",
        "session.set_keyspace(YOUR_KEYSPACE)\n",
        "session"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im1ixFaIJ9Mw",
        "outputId": "42a806a9-0140-4cd5-c056-c621aac467ac"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cassandra.cluster.Session at 0x787085f8f3a0>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_function = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "IeNgg0f1dLkn"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_name = \"pdf\"\n",
        "\n",
        "docsearch = Cassandra.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embedding_function,\n",
        "    session=session,\n",
        "    keyspace=YOUR_KEYSPACE,\n",
        "    table_name=table_name,\n",
        ")"
      ],
      "metadata": {
        "id": "DatB7b8pdYzV"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の操作により、下記のテーブルが作成されます。\n",
        "```\n",
        "CREATE TABLE langchain.pdf (\n",
        "    row_id text PRIMARY KEY,\n",
        "    attributes_blob text,\n",
        "    body_blob text,\n",
        "    metadata_s map<text, text>,\n",
        "    vector vector<float, 1536>\n",
        ") WITH additional_write_policy = '99p'\n",
        "    AND bloom_filter_fp_chance = 0.01\n",
        "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
        "    AND comment = ''\n",
        "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy'}\n",
        "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
        "    AND crc_check_chance = 1.0\n",
        "    AND default_time_to_live = 0\n",
        "    AND gc_grace_seconds = 864000\n",
        "    AND max_index_interval = 2048\n",
        "    AND memtable_flush_period_in_ms = 0\n",
        "    AND min_index_interval = 128\n",
        "    AND read_repair = 'BLOCKING'\n",
        "    AND speculative_retry = '99p';\n",
        "CREATE CUSTOM INDEX eidx_metadata_s_pdf ON langchain.pdf (entries(metadata_s)) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\n",
        "CREATE CUSTOM INDEX idx_vector_pdf ON langchain.pdf (vector) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\n",
        "```\n",
        "\n",
        "Astra DBの「CQL Console」タブで、次のコマンドを実行して上記のDDLを確認することができます。\n",
        "\n",
        "```\n",
        "desc langchain.pdf;\n",
        "```\n",
        "\n",
        "下記のように、登録されたデータの件数を確認することができます。\n",
        "\n",
        "```\n",
        "select count(row_id) from langchain.pdf;\n",
        "```"
      ],
      "metadata": {
        "id": "RyS0PtoMu99M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記で、一度作成済みのテーブルを利用する場合には、引数中の`documents`引数を省略して、以下のように初期化できます。\n",
        "\n",
        "```\n",
        "from langchain.vectorstores import Cassandra\n",
        "docsearch = Cassandra(\n",
        "  embedding=embedding_function,\n",
        "  session=session,\n",
        "  keyspace=YOUR_KEYSPACE,\n",
        "  table_name=\"pdf\",\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "j2SZwxf4v1r2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず、以下では、Astra DBベクトルデータベースに対する、直接のベクトル検索実行を試してみます。"
      ],
      "metadata": {
        "id": "5-162zv9QEJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"RAGのユースケース\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "print(len(docs))\n",
        "for doc in docs:\n",
        "  print(doc)"
      ],
      "metadata": {
        "id": "yK39q3wldnBj",
        "outputId": "1407834d-765e-45d9-af7d-b2c2ff457646",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "page_content='RAG AIアプリケーションの実装\\n検索拡張生成（ RAG）を用いた生成 AIアプリを構築するための、基本的なフロー（図 3）とアーキテクチャダイアグラ\\nム（図 3.1）を紹介しました。ここでは、それらをさらに詳しく見てみましょう。\\nRAGシステムを構築するには、次の 2つのワークフローが必要です。\\n●ナレッジベースの作成\\n●生成プロセスの実行\\n以下、それぞれについて見ていきます。\\nナレッジベースの作成\\nまず、 RAGアプリケーションがアクセスする、エンベディングデータからなるナレッジベースの構築について見てみま\\nしょう。このワークフローには複数のステップがあり、その概要を図 4に示します。\\n図4.データセットをベクトルストアに変換するワークフロー\\n1.プロプライエタリデータの特定\\n初めのステップは、どのようなアプリケーションを構築しているかによって異なってきます。まず、アプリケー' metadata={'page': '9.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='LLMには、それが素のままで用いられた場合、生成された応答が特定のアプリケーションでは用をなさない可能性\\nがある、という限界があります。これは、 LLMに保存されている知識が通常そのモデルのトレーニングに用いられた\\nデータに限定されており、インターネット上で一般に入手できないような、最新のデータや、特定のドメインに関連す\\nるプロプライエタリデータが含まれていないことに由来します。\\n検索拡張生成（ RAG）の中心となるアイディアは、 LLMと、ユーザーが実行を依頼するタスクの解決に関連する独自\\nのドキュメントとを統合することです。\\n図3は、 RAGを用いたアプリケーションがどのように動作するかを示しています。まず、クエリがアプリケーションに対\\nしてサブミットされます。生成 AIアプリでは、クエリの表現は、検索エンジンタイプ、つまり「ジョージ・ワシントンの歴' metadata={'page': '6.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='ず、要は単なる興味深いおもちゃ以上のものにはなり得ません。 RAGは、 LLMに実際にビジネス上の問題を解決す\\nる力を与えます。\\n4' metadata={'page': '3.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='●プロプライエタリデータの活用\\n企業が LLMアプリケーションを開発する場合、 LLMの生成する応答を、その企業の製品 ・サービス とユース\\nケースに対応した内容にするために、そのアプリケーションの要件に関連した企業の保有する情報・ドキュ\\nメントを、 LLMに直接提供することができます。\\n●LLM「トレーニング」の簡便さ\\nRAGパターンは、特定のアプリケーションに合わせてモデルを調整する方法の中でも、実装および保守が\\n容易です。プロプライエタリデータへの新しいドキュメントの追加、既存のドキュメントの変更、または新しい\\nデータソースが利用可能になったときにも、エンベディングデータを更新して、モデルを最新の状態に、そし\\nてその応答を正確なものに保つことが容易です。\\n次の図 3.1は、 RAGを使用したアプリケーションまたはサービスを実行するための完全なアーキテクチャの概要を示' metadata={'page': '7.0', 'source': './DataStax_Vector-Search.pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent\n",
        "\n",
        "LangChainのエージェントを試してみます。"
      ],
      "metadata": {
        "id": "Ba7nukJTeCnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\":1})"
      ],
      "metadata": {
        "id": "3Cf1V6q3eE-G"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.agents.agent_toolkits import create_retriever_tool, create_conversational_retrieval_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.callbacks import StreamlitCallbackHandler\n",
        "from langchain.schema import BaseRetriever, Document, SystemMessage"
      ],
      "metadata": {
        "id": "dgVtEjZEe1mI"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.verbose = True"
      ],
      "metadata": {
        "id": "CYtpk9q-53D9"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "        retriever, \"my_retrevier\", \"Useful when searching for the information about technologies\")\n",
        "tools=[retriever_tool]\n",
        "\n",
        "system_message_content = \"\"\"\n",
        "    Please answer using the given my_retreiver.\n",
        "    \"\"\"\n",
        "system_message_content = f\"{system_message_content} All the responses should be in Japanese language.\"\n",
        "system_message = SystemMessage(content=system_message_content)\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, system_message=system_message, verbose=True, max_tokens_limit=500)"
      ],
      "metadata": {
        "id": "xGysR-omAtVv"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"DATASTAXについて説明して\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pbCG_xPsBGra",
        "outputId": "4925977e-b636-407f-a709-2e79d43cf191"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "my_retrevier: Useful when searching for the information about technologies\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [my_retrevier]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: DATASTAXについて説明して\n",
            "Thought:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m DATASTAXとは何かを知る必要がある\n",
            "Action: my_retrevier\n",
            "Action Input: DATASTAX\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3m[Document(page_content='分散データベース Apache Cassandraの拡張機能であるため、ベクトルとともに、そのベクトルに関連する\\nメタデータを保存しておくことも容易であり、ベクトル検索のみではなく、メタデータを使った操作との組み合\\nわせについても、利点を持っています。このような大規模データ利用のための信頼性・スケールと、データ\\nベースとしての汎用性は、 AstraDBの持つ、単なるベクトルデータベースとは異なる利点です。\\n12', metadata={'page': '11.0', 'source': './DataStax_Vector-Search.pdf'})]\u001b[0m\n",
            "Thought:\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "my_retrevier: Useful when searching for the information about technologies\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [my_retrevier]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: DATASTAXについて説明して\n",
            "Thought: DATASTAXとは何かを知る必要がある\n",
            "Action: my_retrevier\n",
            "Action Input: DATASTAX\n",
            "Observation: [Document(page_content='分散データベース Apache Cassandraの拡張機能であるため、ベクトルとともに、そのベクトルに関連する\\nメタデータを保存しておくことも容易であり、ベクトル検索のみではなく、メタデータを使った操作との組み合\\nわせについても、利点を持っています。このような大規模データ利用のための信頼性・スケールと、データ\\nベースとしての汎用性は、 AstraDBの持つ、単なるベクトルデータベースとは異なる利点です。\\n12', metadata={'page': '11.0', 'source': './DataStax_Vector-Search.pdf'})]\n",
            "Thought:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m DATASTAXの詳細を理解した\n",
            "Final Answer: DATASTAXはApache Cassandraの拡張機能であり、ベクトルデータベースの信頼性とスケール、および汎用性を提供しています。\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DATASTAXはApache Cassandraの拡張機能であり、ベクトルデータベースの信頼性とスケール、および汎用性を提供しています。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記で利用した下記のコードの`k`の数を変更することによって、結果が変わることを確認することもできます。\n",
        "\n",
        "```\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\":1})\n",
        "```"
      ],
      "metadata": {
        "id": "rwGnwJlFQ536"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qN2RLzPn-Wve"
      },
      "execution_count": 112,
      "outputs": []
    }
  ]
}