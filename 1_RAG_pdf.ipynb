{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoshiyukiKono/langchain_for_beginners/blob/main/1_RAG_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf\n",
        "\n",
        "## パッケージインストール"
      ],
      "metadata": {
        "id": "YODRRq-0Y4Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.3.5 tiktoken==0.5.1 cohere==4.36\n",
        "!pip install langchain==0.0.340"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMMMfJaKo4Ww",
        "outputId": "f5dad076-df7c-4405-ba28-dd6645d78a12"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.3.5\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken==0.5.1\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere==4.36\n",
            "  Downloading cohere-4.36-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.3.5) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.3.5)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.5) (4.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (3.8.6)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere==4.36)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere==4.36)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (6.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere==4.36) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.36) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.5) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.5) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.3.5)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.5)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere==4.36) (3.17.0)\n",
            "Installing collected packages: h11, fastavro, backoff, tiktoken, httpcore, httpx, cohere, openai\n",
            "Successfully installed backoff-2.2.1 cohere-4.36 fastavro-1.8.2 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5 tiktoken-0.5.1\n",
            "Collecting langchain==0.0.340\n",
            "  Downloading langchain-0.0.340-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.340)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.340)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.340)\n",
            "  Downloading langsmith-0.0.67-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.340) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.340) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.340) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.340)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.340) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.340) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.340) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.340 langsmith-0.0.67 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cassandra-driver==3.28.0 cassio==0.1.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k10ziT0zRYw",
        "outputId": "2730842b-ad83-4970-815c-98a56c48a216"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cassandra-driver==3.28.0\n",
            "  Downloading cassandra_driver-3.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cassio==0.1.3\n",
            "  Downloading cassio-0.1.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver==3.28.0) (1.16.0)\n",
            "Collecting geomet<0.3,>=0.1 (from cassandra-driver==3.28.0)\n",
            "  Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from cassio==0.1.3) (1.23.5)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.10/dist-packages (from cassio==0.1.3) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver==3.28.0) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2->cassio==0.1.3) (2023.7.22)\n",
            "Installing collected packages: geomet, cassandra-driver, cassio\n",
            "Successfully installed cassandra-driver-3.28.0 cassio-0.1.3 geomet-0.2.1.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf==3.17.0"
      ],
      "metadata": {
        "id": "oX33RRkrY2WY",
        "outputId": "edead122-9b6d-44fc-c269-21b506f07367",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf==3.17.0\n",
            "  Downloading pypdf-3.17.0-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI 接続"
      ],
      "metadata": {
        "id": "IAAChGc-tpsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "WLkDfg9_dB_l",
        "outputId": "f7cbf587-fe77-44b6-e410-b7f4408d7831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "client.models.list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGmRlMyBtZ4n",
        "outputId": "78062773-f8d8-42f5-ee52-4a6628102d16"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncPage[Model](data=[Model(id='text-search-babbage-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'), Model(id='curie-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-003', created=1669599635, object='model', owned_by='openai-internal'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='text-search-babbage-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='babbage', created=1649358449, object='model', owned_by='openai'), Model(id='babbage-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-babbage-001', created=1649364043, object='model', owned_by='openai'), Model(id='text-similarity-davinci-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='davinci-similarity', created=1651172509, object='model', owned_by='openai-dev'), Model(id='code-davinci-edit-001', created=1649880484, object='model', owned_by='openai'), Model(id='curie-similarity', created=1651172510, object='model', owned_by='openai-dev'), Model(id='babbage-search-document', created=1651172510, object='model', owned_by='openai-dev'), Model(id='curie-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-ada-doc-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='davinci-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='text-similarity-babbage-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-search-davinci-doc-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='babbage-similarity', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'), Model(id='davinci-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-similarity-curie-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='text-davinci-001', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-davinci-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='ada-search-document', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-code-search-code', created=1651172505, object='model', owned_by='openai-dev'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='gpt-4-vision-preview', created=1698894917, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='gpt-4-0314', created=1687882410, object='model', owned_by='openai'), Model(id='davinci-search-document', created=1651172509, object='model', owned_by='openai-dev'), Model(id='curie-search-document', created=1651172508, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-code', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'), Model(id='text-search-ada-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='code-search-ada-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-text', created=1651172509, object='model', owned_by='openai-dev'), Model(id='code-search-babbage-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='ada-code-search-text', created=1651172510, object='model', owned_by='openai-dev'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='text-search-curie-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-002', created=1649880484, object='model', owned_by='openai'), Model(id='text-davinci-edit-001', created=1649809179, object='model', owned_by='openai'), Model(id='code-search-babbage-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada', created=1649357491, object='model', owned_by='openai'), Model(id='text-ada-001', created=1649364042, object='model', owned_by='openai'), Model(id='ada-similarity', created=1651172507, object='model', owned_by='openai-dev'), Model(id='code-search-ada-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='text-similarity-ada-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='canary-whisper', created=1699656801, object='model', owned_by='system'), Model(id='text-search-curie-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-curie-001', created=1649364043, object='model', owned_by='openai'), Model(id='curie', created=1649359874, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai'), Model(id='canary-tts', created=1699492935, object='model', owned_by='system'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'), Model(id='davinci', created=1649359874, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-16k-0613', created=1685474247, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system')], object='list')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDFファイルの準備"
      ],
      "metadata": {
        "id": "erSlFwzJo71s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"./DataStax_Vector-Search.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "uqx1XkeMZCMd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "a66f0df3-e720-4816-ca37-ec711eb595c0"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-b2a8f3a4bb8f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./aDataStax_intro.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;34m\"pypdf package not found, please install it with \"\u001b[0m \u001b[0;34m\"`pip install pypdf`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             )\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File path %s is not a valid file or url\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File path ./aDataStax_intro.pdf is not a valid file or url"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "id": "QzK8Pk4RZY5I",
        "outputId": "fa6adaaf-7f8a-49ea-b205-ea60a219b7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Astra DB接続\n"
      ],
      "metadata": {
        "id": "5GmUfFL0YCtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O secure-connect-demo.zip \"https://datastax-cluster-config-...\""
      ],
      "metadata": {
        "id": "IUm51RWBJiU9",
        "outputId": "99d4fd11-d545-4253-b1bf-502da955faa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-30 00:40:33--  https://datastax-cluster-config-prod.s3.us-east-2.amazonaws.com/2c170f07-40f1-4550-b361-6b2c8dabb94f-1/secure-connect-demo.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA2AIQRQ76S2JCB77W%2F20231130%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20231130T003839Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=d5253ef1bd43b488ceae3db174eb2d789b6951ecf8571d894a3ee9ce2be5ee59\n",
            "Resolving datastax-cluster-config-prod.s3.us-east-2.amazonaws.com (datastax-cluster-config-prod.s3.us-east-2.amazonaws.com)... 52.219.110.194, 52.219.178.42, 52.219.94.66, ...\n",
            "Connecting to datastax-cluster-config-prod.s3.us-east-2.amazonaws.com (datastax-cluster-config-prod.s3.us-east-2.amazonaws.com)|52.219.110.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12254 (12K) [application/zip]\n",
            "Saving to: ‘secure-connect-demo.zip’\n",
            "\n",
            "secure-connect-demo 100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-30 00:40:33 (108 MB/s) - ‘secure-connect-demo.zip’ saved [12254/12254]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SECURE_CONNECT_BUNDLE_PATH = 'secure-connect-demo.zip'"
      ],
      "metadata": {
        "id": "qGzwcnhfIROt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "ASTRA_CLIENT_ID = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVlMnfyJJVUb",
        "outputId": "927075df-bf1a-4581-f469-91c021c49e81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ASTRA_CLIENT_SECRET = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvKJFm2vJgoU",
        "outputId": "92378afb-c343-4750-9e16-56fa8d38123c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "cloud_config= {\n",
        "  'secure_connect_bundle': SECURE_CONNECT_BUNDLE_PATH\n",
        "}\n",
        "auth_provider = PlainTextAuthProvider(ASTRA_CLIENT_ID, ASTRA_CLIENT_SECRET)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrGG5yAEJpnJ",
        "outputId": "e2e5f908-8a7a-4ca8-aaf0-50861d360b99"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:f46893af-3674-4086-ad1f-fb43b657e4df. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:f46893af-3674-4086-ad1f-fb43b657e4df. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(132421572132528) 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:f46893af-3674-4086-ad1f-fb43b657e4df> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 2c170f07-40f1-4550-b361-6b2c8dabb94f-us-east1.db.astra.datastax.com:29042:f46893af-3674-4086-ad1f-fb43b657e4df. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0.11-13697dcfc157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain\n",
        "\n",
        "### PDFファイルのスプリット"
      ],
      "metadata": {
        "id": "ZlSnousTt2cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Cassandra\n",
        "from langchain.document_loaders import TextLoader"
      ],
      "metadata": {
        "id": "FCsCyZLJcTgF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "Xv2mLw5ece87"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))\n",
        "print(docs[0])\n",
        "print(docs[len(docs)-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpGh_t-UuD9x",
        "outputId": "8a0ce884-038d-47e6-c5d0-091acc7271c7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "page_content='ホワイトペーパー\\n生成 AIアプリ\\nのための\\nベクトル検索\\nAIアプリケーション開発にベクトル検索を活用す\\nるための開発者/アーキテクト向けガイド\\nこのドキュメントは、生成 AIアプリケーションを企業独\\n自のデータと組み合わせて、設計・構築しようとしてい\\nる全ての方のためのガイドです。組織が理解すべき重\\n要な概念と考慮事項を取り扱うだけでなく、ベクトル検\\n索を用いて、 LLMの持つ機能を大幅に拡張するため\\nのシンプルで強力なアプローチについても解説しま\\nす。' metadata={'source': './DataStax_Vector-Search.pdf', 'page': 0}\n",
            "page_content='20. Karpathy, Andrej. “Andrej Karpathy on Twitter: \"The hottest new programming language is\\nEnglish.\"” Twitter , 2023年 1月24日 ,https://twitter .com/karpathy/status/1617979122625712128\\n21. “MTEB Leaderboard - a Hugging Face Space by mteb.” Hugging Face,\\nhttps://huggingface.co/spaces/mteb/leaderboard\\n22.https://platform.openai.com/docs/guides/embeddings/types-of-embedding-models\\n23. Instructor Text Embedding, https://instructor-embedding.github.io/\\n24. “Libraries - OpenAI API.” Platform OpenAI, https://platform.openai.com/docs/libraries\\n25. “Vertex AI.” Google Cloud, https://cloud.google.com/vertex-ai\\n©2023 DataStax Inc.、全著作権所有。 DataStax は、米国およびその他の国における DataStax, Inc. およびそ\\nの子会社の登録商標です。\\n24' metadata={'source': './DataStax_Vector-Search.pdf', 'page': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ベクトルストアの構築"
      ],
      "metadata": {
        "id": "lWMh_qez4mGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/docs/integrations/vectorstores/cassandra"
      ],
      "metadata": {
        "id": "iVdDamz_dQ2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_KEYSPACE = 'langchain'\n",
        "session.set_keyspace(YOUR_KEYSPACE)\n",
        "session"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im1ixFaIJ9Mw",
        "outputId": "195f97ae-4559-474d-f6aa-cc4517071562"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cassandra.cluster.Session at 0x786fccc1c640>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_function = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "IeNgg0f1dLkn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_name = \"pdf\"\n",
        "\n",
        "docsearch = Cassandra.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_function,\n",
        "    session=session,\n",
        "    keyspace=YOUR_KEYSPACE,\n",
        "    table_name=table_name,\n",
        ")"
      ],
      "metadata": {
        "id": "DatB7b8pdYzV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の操作により、下記のテーブルが作成されます。\n",
        "```\n",
        "CREATE TABLE langchain.pdf (\n",
        "    row_id text PRIMARY KEY,\n",
        "    attributes_blob text,\n",
        "    body_blob text,\n",
        "    metadata_s map<text, text>,\n",
        "    vector vector<float, 1536>\n",
        ") WITH additional_write_policy = '99p'\n",
        "    AND bloom_filter_fp_chance = 0.01\n",
        "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
        "    AND comment = ''\n",
        "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy'}\n",
        "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
        "    AND crc_check_chance = 1.0\n",
        "    AND default_time_to_live = 0\n",
        "    AND gc_grace_seconds = 864000\n",
        "    AND max_index_interval = 2048\n",
        "    AND memtable_flush_period_in_ms = 0\n",
        "    AND min_index_interval = 128\n",
        "    AND read_repair = 'BLOCKING'\n",
        "    AND speculative_retry = '99p';\n",
        "CREATE CUSTOM INDEX eidx_metadata_s_pdf ON langchain.pdf (entries(metadata_s)) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\n",
        "CREATE CUSTOM INDEX idx_vector_pdf ON langchain.pdf (vector) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\n",
        "```\n",
        "\n",
        "Astra DBの「CQL Console」タブで、次のコマンドを実行して上記のDDLを確認することができます。\n",
        "\n",
        "```\n",
        "desc langchain.pdf;\n",
        "```\n",
        "\n",
        "下記のように、登録されたデータの件数を確認することができます。\n",
        "\n",
        "```\n",
        "token@cqlsh:langchain> select count(row_id) from langchain.pdf;\n",
        "\n",
        " system.count(row_id)\n",
        "----------------------\n",
        "                   24\n",
        "\n",
        "(1 rows)\n",
        "```"
      ],
      "metadata": {
        "id": "RyS0PtoMu99M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記で、一度作成済みのテーブルを利用する場合には、引数中の`documents=docs`を省略して、以下のように初期化できます。\n",
        "\n",
        "```\n",
        "from langchain.vectorstores import Cassandra\n",
        "docsearch = Cassandra(\n",
        "  embedding=embedding_function,\n",
        "  session=session,\n",
        "  keyspace=YOUR_KEYSPACE,\n",
        "  table_name=\"pdf\",\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "j2SZwxf4v1r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Cassandra\n",
        "docsearch = Cassandra(\n",
        "  embedding=embedding_function,\n",
        "  session=session,\n",
        "  keyspace=YOUR_KEYSPACE,\n",
        "  table_name=\"pdf\",\n",
        ")"
      ],
      "metadata": {
        "id": "mraUEgHN419j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"RAGのユースケース\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "print(len(docs))\n",
        "for doc in docs:\n",
        "  print(doc)"
      ],
      "metadata": {
        "id": "yK39q3wldnBj",
        "outputId": "c9fec3b6-e7e4-4449-8501-d4fe19260bb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "page_content='RAG AIアプリケーションの実装\\n検索拡張生成（ RAG）を用いた生成 AIアプリを構築するための、基本的なフロー（図 3）とアーキテクチャダイアグラ\\nム（図 3.1）を紹介しました。ここでは、それらをさらに詳しく見てみましょう。\\nRAGシステムを構築するには、次の 2つのワークフローが必要です。\\n●ナレッジベースの作成\\n●生成プロセスの実行\\n以下、それぞれについて見ていきます。\\nナレッジベースの作成\\nまず、 RAGアプリケーションがアクセスする、エンベディングデータからなるナレッジベースの構築について見てみま\\nしょう。このワークフローには複数のステップがあり、その概要を図 4に示します。\\n図4.データセットをベクトルストアに変換するワークフロー\\n1.プロプライエタリデータの特定\\n初めのステップは、どのようなアプリケーションを構築しているかによって異なってきます。まず、アプリケー\\nションで使用できそうなデータセットを検討することになりますが、その際には、そのデータソースはどのよう\\nな固有の情報を提供するか？、エンベディングに利用できる公開データセットはあるか？、といった点を検\\n討する必要があります。\\n公開データセットの中には、 LLM構築時のトレーニングに用いられたデータセットよりも新しいデータが含ま\\nれている可能性があり、新たに、そうしたデータセットを用いることも考えられます。たとえば、最近のニュー\\nス記事や最新の開発者用ドキュメントといったものがあります。\\n2.エンベディングモデルの選択\\nエンベディングモデルの選択において、ナレッジベース用のエンベディングモデルと最終的な応答のテキス\\n10' metadata={'page': '9.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='RAG AIアプリケーションの実装\\n検索拡張生成（ RAG）を用いた生成 AIアプリを構築するための、基本的なフロー（図 3）とアーキテクチャダイアグラ\\nム（図 3.1）を紹介しました。ここでは、それらをさらに詳しく見てみましょう。\\nRAGシステムを構築するには、次の 2つのワークフローが必要です。\\n●ナレッジベースの作成\\n●生成プロセスの実行\\n以下、それぞれについて見ていきます。\\nナレッジベースの作成\\nまず、 RAGアプリケーションがアクセスする、エンベディングデータからなるナレッジベースの構築について見てみま\\nしょう。このワークフローには複数のステップがあり、その概要を図 4に示します。\\n図4.データセットをベクトルストアに変換するワークフロー\\n1.プロプライエタリデータの特定\\n初めのステップは、どのようなアプリケーションを構築しているかによって異なってきます。まず、アプリケー\\nションで使用できそうなデータセットを検討することになりますが、その際には、そのデータソースはどのよう\\nな固有の情報を提供するか？、エンベディングに利用できる公開データセットはあるか？、といった点を検\\n討する必要があります。\\n公開データセットの中には、 LLM構築時のトレーニングに用いられたデータセットよりも新しいデータが含ま\\nれている可能性があり、新たに、そうしたデータセットを用いることも考えられます。たとえば、最近のニュー\\nス記事や最新の開発者用ドキュメントといったものがあります。\\n2.エンベディングモデルの選択\\nエンベディングモデルの選択において、ナレッジベース用のエンベディングモデルと最終的な応答のテキス\\n10' metadata={'page': '9.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='最後に、アプリは検索に該当した上位数件のデータのテキストを取得し、それらを LLMに与える命令文の作成に活\\n用して、 LLMにタスクの結果を生成させます。\\n図3.RAGパターンを用いた LLMアプリケーションの基本フロー\\nこのアプローチには、次の利点があります。\\n●ハルシネーションの軽減\\nAIの分野では、一見すると尤もらしく見えるものの、事実とは異なる、あるいは文脈に適さない回答が出力\\nされることを「ハルシネーション（ Hallucination：幻覚）」と呼びます。 RAGデザインパターンは、ベクトル検\\n索によって抽出されたドキュメントに焦点を当てるように LLMに指示して、応答を生成します。 LLMのト\\nレーニングに用いられたデータセットの全体ではなく、より文脈に適したドキュメントセットを使用することで、\\nクエリへの応答は、ユーザーの意図をより正確に汲み取ったものになります。\\n●プロプライエタリデータの活用\\n企業が LLMアプリケーションを開発する場合、 LLMの生成する応答を、その企業の製品 ・サービス とユース\\nケースに対応した内容にするために、そのアプリケーションの要件に関連した企業の保有する情報・ドキュ\\nメントを、 LLMに直接提供することができます。\\n●LLM「トレーニング」の簡便さ\\nRAGパターンは、特定のアプリケーションに合わせてモデルを調整する方法の中でも、実装および保守が\\n容易です。プロプライエタリデータへの新しいドキュメントの追加、既存のドキュメントの変更、または新しい\\nデータソースが利用可能になったときにも、エンベディングデータを更新して、モデルを最新の状態に、そし\\nてその応答を正確なものに保つことが容易です。\\n次の図 3.1は、 RAGを使用したアプリケーションまたはサービスを実行するための完全なアーキテクチャの概要を示\\nしています。このアーキテクチャでは、エージェントという新たな概念が導入されています。エージェントとは、自動化\\nされた推論・意思決定エンジンです。エージェントは、ユーザーからの入力すなわちクエリを受け取り、それに応じて\\n何を行うかを決定するアプリケーション・コンポーネントまたはプログラミング・コードです。\\n図3で紹介した単純な RAGの例では、エージェントのタスクは、ユーザークエリのエンベディング、ベクトル検索の実\\n行、検索結果を含む LLMのプロンプトの作成といった一連の基本的なプロセスからなっていました。\\n8' metadata={'page': '7.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='より複雑なアプリケーションでは、タスクを複数のサブタスクに分割した上で、問題を解決するために外部ツールを使\\n用する必要があるかどうかを判断したり、アプリケーション用のメモリを使ってデータを保存し維持するために、エー\\nジェントが利用される場合があります。\\n図3.1RAGパターンを用いたアーキテクチャのコンテキストビュー\\nここで、エンベディングデータの保存とベクトル検索の実行にベクトル対応データベースを採用すると、利用可能な\\nデータ量を非常に大規模なデータセットに拡張しながら、ドキュメント検索速度の性能を維持できるという利点を得る\\nことができます。ベクトル対応データベースは、あらゆる生成エクスペリエンスの基本要件である、類似性検索を高\\nい効率で実現する機能を備えています。\\nベクトル対応データベースの採用\\nSequoia Capital 社の最近の調査10によると、生成 AIに取り組む 88%の企業が、生成 AIアプリ\\nケーションの実現に向けて、ドキュメント検索メカニズムが、今後インフラストラクチャを構成する\\n重要な部分になると考えています。\\n9' metadata={'page': '8.0', 'source': './DataStax_Vector-Search.pdf'}\n",
            "page_content='とになります。それは例えば、 AI安全性分析、品質管理解析、そして LLMからの応答に対してアプリケー\\nションのユースケースに固有の付加的なフィルタリングやアジャストメントなどを組み込むというものです。\\n多くのシステムでは、この後処理のステージを、生成された応答を、後で集合的にレビューするために、そ\\nのコピーをシステムに保存する機会としても使用します。 LLM応答の後処理が完了すると、最終的な応答\\nがユーザーに返され、一連の RAGプロセスの全体が完了します。\\n16' metadata={'page': '15.0', 'source': './DataStax_Vector-Search.pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "Ba7nukJTeCnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\":1})"
      ],
      "metadata": {
        "id": "3Cf1V6q3eE-G"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.agents.agent_toolkits import create_retriever_tool, create_conversational_retrieval_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.callbacks import StreamlitCallbackHandler\n",
        "from langchain.schema import BaseRetriever, Document, SystemMessage"
      ],
      "metadata": {
        "id": "dgVtEjZEe1mI"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.verbose = True"
      ],
      "metadata": {
        "id": "CYtpk9q-53D9"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "        retriever, \"my_retrevier\", \"Useful when searching for the information about technologies\")\n",
        "tools=[retriever_tool]\n",
        "\n",
        "system_message_content = \"\"\"\n",
        "    Please answer using the given my_retreiver.\n",
        "    \"\"\"\n",
        "system_message_content = f\"{system_message_content} All the responses should be in Japanese language.\"\n",
        "system_message = SystemMessage(content=system_message_content)\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, system_message=system_message, verbose=True, max_tokens_limit=500)"
      ],
      "metadata": {
        "id": "xGysR-omAtVv"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"DATASTAXについて説明して\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "pbCG_xPsBGra",
        "outputId": "ca7f91e6-999f-454a-be21-446501274abe"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m DataStaxに関する情報を探す\n",
            "Action: my_retrevier\n",
            "Action Input: DataStax\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3m[Document(page_content='カテゴリー情報を利用することができます。\\n4.エンベディングデータの生成と保存\\n先に、エンベディングに用いるためにモデルを選択しました。このモデルを用いてベクトルを生成することに\\nなります。\\nこのプロセスの最初のステップは、テキストのチャンク化です。これは、テキストをより短い文字列に分割す\\nる方法を決定するプロセスです。これは主に次の２つの理由から行われます。\\nまず、エンベディングモデルには入力として受け入れることのできるテキストの量に制限があるためです。\\ntext-ada-002 の場合、その制限は 8,192トークンです。受け入れられる文字列の長さがより短いモデルも\\nあります。\\nまた、チャンク化要否の判断は、モデルが持つ制限以外にも、データソースとアプリケーションの両方の面\\nからも検討する必要があります。アプリケーションがドキュメント全体をユーザーに返す必要がある場合、モ\\nデルが許す限り、できるだけ大きなチャンクを用いることは合理的です。あるいは、アプリケーションが、例\\nえば質疑応答システムの拡張といった場合のように、様々な事実に依存している場合、ドキュメント全体よ\\nりも、その中の数センテンス程度の短いチャンクの方が、一つのトピックに関する事実やアイデアを扱って\\nいる可能性が高く、このユースケースにより適しています。\\nさて、チャンクの長さが決まったら、次はチャンク化に関する戦略を立てる必要があります。これには多くの\\n方法があります。\\n最も簡単な方法は、テキストを均等な長さのチャンクに分割することです。この場合、重要な事実を説明し\\nている途中でチャンクが単語や文を分割してしまうことが生じる可能性があるため、チャンクのオーバーラッ\\nプを許容すると文書内の概念を適切に捉えるのに役立ちます。\\nより高度な、広く用いられている戦略として、ドキュメントの、または言語の構造に基づいたチャンク化があ\\nります。例としては、ドキュメントのセクション、段落ごと、またはセンテンスの境界を基準にしたチャンク化が\\n挙げられます。\\nドキュメントのチャンクのセットを作成したら、各チャンクのベクトルを、先の手順で定義した関連メタデータと\\n共に、データストアに保存します。これにより、近似最近傍探索（ ANN: Approximate Nearest Neighbor\\n）を利用した高速ベクトル検索が可能になります。 DataStaxの Apache Cassandraマネージドサービスで\\nあるAstraDB は、ベクトル検索の機能を有しており、大規模なベクトルデータセットを操作する場合の優れ\\nた選択肢となります。そしてまた、 AstraDBのベクトル検索機能は、高い可用性と柔軟なスケールを有した\\n分散データベース Apache Cassandraの拡張機能であるため、ベクトルとともに、そのベクトルに関連する\\nメタデータを保存しておくことも容易であり、ベクトル検索のみではなく、メタデータを使った操作との組み合\\nわせについても、利点を持っています。このような大規模データ利用のための信頼性・スケールと、データ\\nベースとしての汎用性は、 AstraDBの持つ、単なるベクトルデータベースとは異なる利点です。\\n12', metadata={'page': '11.0', 'source': './DataStax_Vector-Search.pdf'})]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m DataStaxとは、Apache CassandraマネージドサービスであるAstraDBを利用して、近似最近傍探索（ANN）を利用した高速ベクトル検索を行えるサービスであることがわかった。\n",
            "Final Answer: DataStaxは、Apache CassandraマネージドサービスであるAstraDBを利用して、近似最近傍探索（ANN）を利用した高速ベクトル検索を提供しているサービスです。\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DataStaxは、Apache CassandraマネージドサービスであるAstraDBを利用して、近似最近傍探索（ANN）を利用した高速ベクトル検索を提供しているサービスです。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qN2RLzPn-Wve"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}